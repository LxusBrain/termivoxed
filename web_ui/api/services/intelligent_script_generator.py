"""
Intelligent Script Generator - Complete AI-powered script generation pipeline

This module orchestrates the entire script generation process:
1. Prompt Engineering → Generate sophisticated prompts
2. LLM Generation → Call the AI to generate scripts
3. Analysis → Validate timing and quality
4. Refinement → Iteratively improve scripts that don't fit
5. Progress → Stream detailed updates to the frontend

The goal is to produce production-ready scripts that:
- Fit precisely within segment durations
- Match user's creative vision
- Sound natural when spoken
- Require minimal manual editing
"""

import asyncio
import json
import re
from typing import List, Dict, Optional, Tuple, Any, AsyncGenerator, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

from web_ui.api.services.prompt_engineer import (
    PromptEngineer,
    VideoContext,
    StyleProfile,
    SegmentSpec,
    RefinementFeedback,
    PacingStyle,
    prompt_engineer,
)
from web_ui.api.services.script_analyzer import (
    ScriptAnalyzer,
    FullAnalysis,
    SegmentAnalysis,
    FitStatus,
    script_analyzer,
)
from web_ui.api.services.llm_service import (
    LLMService,
    BaseLLMProvider,
    OllamaProvider,
    VideoSegmentList,
    llm_service,
)
from web_ui.api.schemas.llm_schemas import LLMProvider, AutoGeneratedSegment

from utils.logger import logger


class GenerationStage(Enum):
    """Stages of the generation pipeline"""
    INITIALIZING = "initializing"
    ANALYZING_VIDEO = "analyzing_video"
    CRAFTING_PROMPT = "crafting_prompt"
    GENERATING = "generating"
    PARSING_RESPONSE = "parsing_response"
    VALIDATING = "validating"
    REFINING = "refining"
    FINALIZING = "finalizing"
    COMPLETED = "completed"
    ERROR = "error"


@dataclass
class GenerationProgress:
    """Progress update for the frontend"""
    stage: GenerationStage
    message: str
    progress: int  # 0-100
    detail: Optional[str] = None
    segment_count: int = 0
    segments_validated: int = 0
    segments_refined: int = 0
    current_refinement_attempt: int = 0
    max_refinement_attempts: int = 3
    warnings: List[str] = field(default_factory=list)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class GeneratedSegmentResult:
    """Result for a single generated segment"""
    index: int
    name: str
    start_time: float
    end_time: float
    script: str
    description: str
    word_count: int
    estimated_duration: float
    fits_duration: bool
    fit_percentage: float
    refinement_applied: bool = False
    refinement_iterations: int = 0


@dataclass
class GenerationResult:
    """Complete result of script generation"""
    success: bool
    segments: List[GeneratedSegmentResult]
    total_segments: int
    segments_fitting: int
    overall_coverage: float
    all_fit: bool
    warnings: List[str]
    provider_used: str
    model_used: str
    total_duration: float
    generation_time: float  # seconds
    refinement_iterations: int


class IntelligentScriptGenerator:
    """
    Orchestrates intelligent script generation with iterative refinement.

    Pipeline:
    1. Analyze video context
    2. Craft optimized prompt using PromptEngineer
    3. Generate scripts via LLM
    4. Validate scripts using ScriptAnalyzer
    5. Refine scripts that don't fit (up to max_iterations)
    6. Return production-ready scripts
    """

    def __init__(
        self,
        prompt_engineer: PromptEngineer = None,
        script_analyzer: ScriptAnalyzer = None,
        llm_service: LLMService = None
    ):
        self.prompt_engineer = prompt_engineer or PromptEngineer()
        self.script_analyzer = script_analyzer or ScriptAnalyzer()
        self.llm_service = llm_service or LLMService()

    async def generate_auto_segments(
        self,
        provider_config: LLMProvider,
        video_duration: float,
        video_description: str,
        video_title: Optional[str] = None,
        style_config: Optional[Dict] = None,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0,
        custom_instructions: Optional[str] = None,
        max_refinement_iterations: int = 3,
        progress_callback: Optional[Callable[[GenerationProgress], None]] = None,
        video_path: Optional[str] = None,
        vision_model: Optional[str] = None,
        use_video_analysis: bool = False
    ) -> GenerationResult:
        """
        Generate segments with scripts automatically from video description.

        This is the main entry point for full AI-powered generation.

        Args:
            provider_config: LLM provider configuration
            video_duration: Duration of video in seconds
            video_description: Text description of video content
            video_title: Optional title of the video
            style_config: Style configuration (tone, style, audience, language)
            min_segment_duration: Minimum segment duration in seconds
            max_segment_duration: Maximum segment duration in seconds
            custom_instructions: Additional custom instructions for generation
            max_refinement_iterations: Max attempts to refine scripts that don't fit
            progress_callback: Callback for progress updates
            video_path: Optional path to video file for AI vision analysis
            vision_model: Vision model to use (default: qwen3-vl:8b)
            use_video_analysis: Whether to use AI video analysis to understand content
        """
        start_time = datetime.now()
        warnings = []

        # Setup progress reporting
        def report_progress(stage: GenerationStage, message: str, progress: int, **kwargs):
            if progress_callback:
                update = GenerationProgress(
                    stage=stage,
                    message=message,
                    progress=progress,
                    **kwargs
                )
                progress_callback(update)
            logger.info(f"[{stage.value}] {message}")

        try:
            # Stage 1: Initialize and analyze
            report_progress(
                GenerationStage.INITIALIZING,
                "Initializing script generation...",
                5
            )

            # Build video context
            video_context = VideoContext(
                duration=video_duration,
                title=video_title,
                description=video_description
            )

            # Build style profile
            style = StyleProfile(
                tone=style_config.get("tone", "documentary") if style_config else "documentary",
                style=style_config.get("style", "narrative") if style_config else "narrative",
                audience=style_config.get("audience", "general") if style_config else "general",
                language=style_config.get("language", "en") if style_config else "en",
                pacing=PacingStyle.MODERATE,
            )

            # Stage 2: Analyze video (with optional AI vision analysis)
            report_progress(
                GenerationStage.ANALYZING_VIDEO,
                f"Analyzing video: {video_duration:.1f}s, detecting content type...",
                10,
                detail=f"Title: {video_title or 'Untitled'}"
            )

            # AI Video Analysis (optional - enhances understanding of video content)
            ai_video_description = None
            if use_video_analysis and video_path:
                try:
                    report_progress(
                        GenerationStage.ANALYZING_VIDEO,
                        "Analyzing video content with AI vision model...",
                        12,
                        detail=f"Using {vision_model or 'qwen3-vl:8b'} to understand video"
                    )

                    # Create an OllamaProvider for video analysis
                    vision_provider = OllamaProvider(model=vision_model or "qwen3-vl:8b")

                    # Analyze the video
                    analysis_result = await vision_provider.analyze_video(
                        video_path=video_path,
                        num_frames=4,
                        vision_model=vision_model or "qwen3-vl:8b"
                    )

                    if analysis_result["success"]:
                        ai_video_description = analysis_result["description"]
                        logger.info(f"AI video analysis successful: {len(ai_video_description)} chars")

                        report_progress(
                            GenerationStage.ANALYZING_VIDEO,
                            "AI video analysis complete!",
                            15,
                            detail=f"Analyzed {analysis_result['frames_analyzed']} frames"
                        )

                        # Enhance the video description with AI analysis
                        if ai_video_description:
                            enhanced_description = f"""USER PROVIDED DESCRIPTION:
{video_description}

AI VIDEO ANALYSIS:
{ai_video_description}

Use the AI video analysis to understand what's actually shown in the video, and combine it with the user's description to create accurate, engaging narration scripts."""
                            video_description = enhanced_description
                            video_context = VideoContext(
                                duration=video_duration,
                                title=video_title,
                                description=video_description
                            )
                    else:
                        logger.warning(f"AI video analysis failed: {analysis_result.get('error', 'Unknown error')}")
                        warnings.append(f"Video analysis skipped: {analysis_result.get('error', 'Unknown error')}")

                except Exception as e:
                    logger.warning(f"AI video analysis error (non-fatal): {e}")
                    warnings.append(f"Video analysis unavailable: {str(e)}")

            # Detect video type
            content_type = self.prompt_engineer.detect_video_type(
                video_description,
                video_title
            )

            await asyncio.sleep(0.1)  # Brief pause for UI update

            # Stage 3: Craft prompt
            report_progress(
                GenerationStage.CRAFTING_PROMPT,
                f"Crafting optimized prompt for {content_type.value} content...",
                15,
                detail=f"Style: {style.tone}, Audience: {style.audience}"
            )

            prompt = self.prompt_engineer.build_auto_segment_prompt(
                video_context=video_context,
                style=style,
                min_segment_duration=min_segment_duration,
                max_segment_duration=max_segment_duration,
                custom_instructions=custom_instructions
            )

            # Stage 4: Generate with LLM
            report_progress(
                GenerationStage.GENERATING,
                f"Generating scripts with {provider_config.type}/{provider_config.model}...",
                25,
                detail="This may take a moment..."
            )

            provider = self.llm_service.get_provider(provider_config)

            # Use structured outputs for Ollama (more reliable JSON parsing)
            segments = None
            if isinstance(provider, OllamaProvider):
                try:
                    logger.info("Using Ollama structured outputs for reliable JSON generation")

                    # Build a simplified prompt optimized for structured output
                    # The format schema will enforce the structure
                    structured_prompt = self._build_structured_output_prompt(
                        video_context=video_context,
                        style=style,
                        min_segment_duration=min_segment_duration,
                        max_segment_duration=max_segment_duration,
                        custom_instructions=custom_instructions
                    )

                    result = await provider.generate_structured(
                        prompt=structured_prompt,
                        response_schema=VideoSegmentList,
                        max_tokens=4000,
                        temperature=0.5  # Lower temperature for more consistent output
                    )
                    # Convert Pydantic models to dicts
                    segments = [
                        {
                            "name": seg.name,
                            "start_time": seg.start_time,
                            "end_time": seg.end_time,
                            "description": seg.description,
                            "script": seg.script,
                            "word_count": len(seg.script.split())
                        }
                        for seg in result.segments
                    ]
                    logger.info(f"Structured output: {len(segments)} segments generated successfully")
                except Exception as e:
                    logger.warning(f"Structured output failed, falling back to standard generation: {e}")
                    # Fall back to standard generation
                    segments = None

            # Standard generation (fallback or non-Ollama providers)
            if segments is None:
                response = await provider.generate(prompt, max_tokens=4000, temperature=0.7)
                # Stage 5: Parse response
                report_progress(
                    GenerationStage.PARSING_RESPONSE,
                    "Parsing AI response...",
                    50
                )
                segments = self._parse_segment_response(response, video_duration)
            else:
                # Stage 5: Skip parsing for structured outputs
                report_progress(
                    GenerationStage.PARSING_RESPONSE,
                    "Structured output received, validating...",
                    50
                )

            if not segments:
                raise ValueError("Failed to parse any segments from AI response")

            report_progress(
                GenerationStage.PARSING_RESPONSE,
                f"Parsed {len(segments)} segments from response",
                55,
                segment_count=len(segments)
            )

            # Stage 6: Validate
            report_progress(
                GenerationStage.VALIDATING,
                "Validating segment timing and scripts...",
                60,
                segment_count=len(segments)
            )

            # Prepare segments for analysis
            segments_for_analysis = [
                {
                    "script": seg.get("script", ""),
                    "start_time": seg.get("start_time", 0),
                    "end_time": seg.get("end_time", 0),
                }
                for seg in segments
            ]

            # Run analysis
            self.script_analyzer.language = style.language
            analysis = self.script_analyzer.analyze_all_segments(
                segments_for_analysis,
                video_duration
            )

            segments_needing_refinement = [
                (i, seg, analysis.segments[i])
                for i, seg in enumerate(segments)
                if analysis.segments[i].needs_refinement
            ]

            report_progress(
                GenerationStage.VALIDATING,
                f"Validation complete: {analysis.scripts_fitting}/{len(segments)} scripts fit well",
                65,
                segment_count=len(segments),
                segments_validated=len(segments),
                warnings=analysis.all_issues[:3] if analysis.all_issues else []
            )

            # Stage 7: Refinement loop
            total_refinements = 0
            if segments_needing_refinement and max_refinement_iterations > 0:
                report_progress(
                    GenerationStage.REFINING,
                    f"Refining {len(segments_needing_refinement)} segment(s) that need adjustment...",
                    70,
                    segment_count=len(segments),
                    segments_validated=len(segments) - len(segments_needing_refinement)
                )

                for iteration in range(max_refinement_iterations):
                    if not segments_needing_refinement:
                        break

                    report_progress(
                        GenerationStage.REFINING,
                        f"Refinement iteration {iteration + 1}/{max_refinement_iterations}",
                        70 + (iteration * 8),
                        current_refinement_attempt=iteration + 1,
                        max_refinement_attempts=max_refinement_iterations,
                        segments_refined=total_refinements
                    )

                    # Parallel refinement: process multiple segments concurrently
                    async def refine_single_segment(idx, seg, seg_analysis):
                        """Refine a single segment and return result"""
                        try:
                            # Build refinement feedback
                            feedback = RefinementFeedback(
                                segment_index=idx,
                                issue_type="too_long" if seg_analysis.overflow_seconds > 0 else "too_short",
                                current_duration=seg_analysis.estimated_duration,
                                target_duration=seg_analysis.segment_duration,
                                current_word_count=seg_analysis.word_count,
                                target_word_count=seg_analysis.target_word_count,
                                specific_feedback=f"Script needs to be {'shorter' if seg_analysis.overflow_seconds > 0 else 'longer'}"
                            )

                            # Build refinement prompt
                            refinement_prompt = self.prompt_engineer.build_refinement_prompt(
                                seg["script"],
                                feedback,
                                style
                            )

                            refined_response = await provider.generate(
                                refinement_prompt,
                                max_tokens=500,
                                temperature=0.6
                            )

                            refined_script = refined_response.strip()

                            # Re-analyze
                            new_analysis = self.script_analyzer.analyze_segment(
                                refined_script,
                                seg["start_time"],
                                seg["end_time"],
                                idx
                            )

                            return (idx, seg, seg_analysis, refined_script, new_analysis, None)

                        except Exception as e:
                            logger.warning(f"Refinement failed for segment {idx}: {e}")
                            return (idx, seg, seg_analysis, None, None, str(e))

                    # Run refinements in parallel (limit concurrency to avoid overwhelming the LLM)
                    # Process in batches of 3 to balance speed and resource usage
                    batch_size = 3
                    still_needing = []

                    for batch_start in range(0, len(segments_needing_refinement), batch_size):
                        batch = segments_needing_refinement[batch_start:batch_start + batch_size]
                        tasks = [refine_single_segment(idx, seg, analysis) for idx, seg, analysis in batch]
                        results = await asyncio.gather(*tasks, return_exceptions=True)

                        for result in results:
                            if isinstance(result, Exception):
                                logger.error(f"Unexpected refinement error: {result}")
                                continue

                            idx, seg, seg_analysis, refined_script, new_analysis, error = result

                            if error:
                                # Refinement failed, keep in queue for next iteration
                                still_needing.append((idx, seg, seg_analysis))
                            elif refined_script and new_analysis:
                                # Check if improved
                                if new_analysis.fit_percentage > seg_analysis.fit_percentage:
                                    segments[idx]["script"] = refined_script
                                    segments[idx]["word_count"] = new_analysis.word_count
                                    segments[idx]["refined"] = True
                                    total_refinements += 1

                                    if new_analysis.needs_refinement:
                                        still_needing.append((idx, segments[idx], new_analysis))
                                else:
                                    # Refinement didn't help, keep original
                                    still_needing.append((idx, seg, seg_analysis))

                    segments_needing_refinement = still_needing

                report_progress(
                    GenerationStage.REFINING,
                    f"Refinement complete: {total_refinements} segment(s) improved",
                    90,
                    segments_refined=total_refinements
                )

            # Stage 8: Finalize
            report_progress(
                GenerationStage.FINALIZING,
                "Finalizing results...",
                95
            )

            # Re-analyze final state
            final_segments_for_analysis = [
                {
                    "script": seg.get("script", ""),
                    "start_time": seg.get("start_time", 0),
                    "end_time": seg.get("end_time", 0),
                }
                for seg in segments
            ]
            final_analysis = self.script_analyzer.analyze_all_segments(
                final_segments_for_analysis,
                video_duration
            )

            # Build results
            results = []
            for i, seg in enumerate(segments):
                seg_analysis = final_analysis.segments[i]
                results.append(GeneratedSegmentResult(
                    index=i,
                    name=seg.get("name", f"Segment {i + 1}"),
                    start_time=seg.get("start_time", 0),
                    end_time=seg.get("end_time", 0),
                    script=seg.get("script", ""),
                    description=seg.get("description", ""),
                    word_count=seg_analysis.word_count,
                    estimated_duration=seg_analysis.estimated_duration,
                    fits_duration=seg_analysis.fit_status in [FitStatus.PERFECT, FitStatus.GOOD, FitStatus.ACCEPTABLE],
                    fit_percentage=seg_analysis.fit_percentage,
                    refinement_applied=seg.get("refined", False),
                    refinement_iterations=total_refinements
                ))

            # Collect warnings
            for issue in final_analysis.all_issues:
                if issue.severity in ["high", "critical"]:
                    warnings.append(issue.description)

            generation_time = (datetime.now() - start_time).total_seconds()

            report_progress(
                GenerationStage.COMPLETED,
                f"Generation complete: {len(results)} segments, {final_analysis.scripts_fitting} fit well",
                100,
                segment_count=len(results),
                segments_validated=final_analysis.scripts_fitting,
                segments_refined=total_refinements
            )

            return GenerationResult(
                success=True,
                segments=results,
                total_segments=len(results),
                segments_fitting=final_analysis.scripts_fitting,
                overall_coverage=final_analysis.overall_coverage,
                all_fit=final_analysis.all_fit,
                warnings=warnings,
                provider_used=provider_config.type,
                model_used=provider_config.model,
                total_duration=sum(r.end_time - r.start_time for r in results),
                generation_time=generation_time,
                refinement_iterations=total_refinements
            )

        except Exception as e:
            error_msg = str(e) if str(e) else f"{type(e).__name__}: Unknown error"
            logger.error(f"Script generation failed: {error_msg}")
            report_progress(
                GenerationStage.ERROR,
                f"Generation failed: {error_msg}",
                0
            )
            # Re-raise with better error message
            raise RuntimeError(f"Script generation failed: {error_msg}") from e

    def _build_structured_output_prompt(
        self,
        video_context: VideoContext,
        style: StyleProfile,
        min_segment_duration: float,
        max_segment_duration: float,
        custom_instructions: Optional[str] = None
    ) -> str:
        """
        Build a simplified prompt optimized for structured JSON output.
        This prompt is designed to work with Ollama's format parameter.
        """
        # Calculate number of segments
        avg_duration = (min_segment_duration + max_segment_duration) / 2
        num_segments = max(1, int(video_context.duration / avg_duration))

        # Word count guide
        words_per_second = 2.5  # Average speaking rate

        # Calculate expected word counts
        segment_duration = (min_segment_duration + max_segment_duration) / 2
        words_per_segment = int(segment_duration * words_per_second)

        prompt = f"""You are a professional video scriptwriter. Create exactly {num_segments} video segments for a {video_context.duration:.0f} second video.

VIDEO TITLE: {video_context.title or 'Untitled'}
VIDEO DESCRIPTION: {video_context.description}
TONE: {style.tone}
STYLE: {style.style}

CRITICAL REQUIREMENTS:
1. Create exactly {num_segments} segments
2. Segments must cover 0 to {video_context.duration} seconds (the FULL video)
3. Each segment: {min_segment_duration}-{max_segment_duration} seconds duration
4. Each script must have {words_per_segment}-{words_per_segment + 10} words (IMPORTANT!)

SCRIPT WORD COUNT:
- Speaking rate is {words_per_second:.1f} words per second
- A {segment_duration:.0f}-second segment needs approximately {words_per_segment} words
- Scripts that are too short will NOT fill the segment duration
- Write LONGER scripts with more detail and description

EXAMPLE of a proper {segment_duration:.0f}-second script ({words_per_segment} words):
"Welcome to today's cosmic journey where we explore the mysteries of the universe and reveal what the stars have aligned for you. The celestial bodies are moving in fascinating patterns that will influence your day in unexpected ways."

{f'ADDITIONAL INSTRUCTIONS: {custom_instructions}' if custom_instructions else ''}

Write engaging, detailed narration for voice-over. Make each script the appropriate length for its duration."""

        return prompt

    def _parse_segment_response(self, response: str, video_duration: float) -> List[Dict]:
        """Parse LLM response into segment data"""

        # Log raw response for debugging
        logger.debug(f"Raw LLM response ({len(response)} chars): {response[:500]}...")

        # Clean response
        cleaned = self._clean_response(response)
        logger.debug(f"Cleaned response ({len(cleaned)} chars): {cleaned[:500]}...")

        # If cleaned is too short, the model didn't generate valid content
        if len(cleaned.strip()) < 10:
            logger.warning(f"Cleaned response too short: '{cleaned}'")
            # Check if there's JSON content after think tags
            # Try extracting JSON from the original response
            json_match = re.search(r'\[[\s\S]*?\]', response)
            if json_match:
                cleaned = json_match.group()
                logger.info(f"Extracted JSON from raw response: {cleaned[:200]}...")

        # The prompt ends with "[" so the response might start right after that
        # If it doesn't start with [ or {, try to find where JSON starts
        cleaned_stripped = cleaned.strip()
        if not cleaned_stripped.startswith('[') and not cleaned_stripped.startswith('{'):
            # Try to find JSON array in the response
            json_start = cleaned.find('[')
            if json_start >= 0:
                cleaned = cleaned[json_start:]
            else:
                # Prepend [ if no JSON found
                cleaned = '[' + cleaned

        try:
            # Find JSON array
            json_match = re.search(r'\[[\s\S]*\]', cleaned)
            if json_match:
                data = json.loads(json_match.group())
                segments = []

                for i, item in enumerate(data):
                    if isinstance(item, dict):
                        start_time = float(item.get("start_time", 0))
                        end_time = float(item.get("end_time", start_time + 10))

                        # Validate and clamp times
                        start_time = max(0, min(start_time, video_duration))
                        end_time = max(start_time + 1, min(end_time, video_duration))

                        script_text = item.get("script", item.get("text", ""))
                        segments.append({
                            "name": item.get("name", f"Segment {i + 1}"),
                            "start_time": start_time,
                            "end_time": end_time,
                            "description": item.get("description", ""),
                            "script": script_text,
                            "word_count": item.get("word_count", len(script_text.split())),
                            "reasoning": item.get("reasoning", "")
                        })

                if segments:
                    # Final check: warn if scripts look like placeholders
                    all_scripts = ' '.join(s.get('script', '') for s in segments).lower()
                    if 'your narration' in all_scripts or 'script here' in all_scripts:
                        logger.warning(f"Parsed segments may contain placeholder text! Scripts: {all_scripts[:200]}...")
                    else:
                        logger.info(f"Successfully parsed {len(segments)} segments with real content")
                    return segments

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")

        # Fallback: single segment
        logger.warning("Failed to parse segments, creating fallback")
        return [{
            "name": "Full Video",
            "start_time": 0,
            "end_time": video_duration,
            "description": "Full video narration",
            "script": cleaned[:500] if len(cleaned) > 500 else cleaned,
            "word_count": len(cleaned.split())
        }]

    def _clean_response(self, response: str) -> str:
        """
        Clean LLM response - remove thinking tags, artifacts, and extract JSON

        Handles various model output formats:
        - <think>...</think> blocks (qwen3, deepseek-r1, qwq)
        - Markdown code blocks
        - Special tokens
        """
        if not response:
            return ""

        logger.info(f"Cleaning response of length {len(response)}")

        def try_extract_json_array(text: str) -> Optional[str]:
            """Try to extract a valid JSON array from text.

            Returns the BEST candidate (longest script content) to avoid
            returning example/placeholder JSON from the prompt.
            """
            # Collect all valid candidates
            candidates = []
            start_pos = 0

            while True:
                bracket_start = text.find('[', start_pos)
                if bracket_start == -1:
                    break

                # Find matching closing bracket by counting
                depth = 0
                in_string = False
                escape_next = False
                bracket_end = -1

                for i in range(bracket_start, len(text)):
                    char = text[i]

                    if escape_next:
                        escape_next = False
                        continue

                    if char == '\\' and in_string:
                        escape_next = True
                        continue

                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue

                    if in_string:
                        continue

                    if char == '[':
                        depth += 1
                    elif char == ']':
                        depth -= 1
                        if depth == 0:
                            bracket_end = i + 1
                            break

                if bracket_end > bracket_start:
                    candidate = text[bracket_start:bracket_end]
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, list) and len(parsed) > 0:
                            # Check if it looks like segment data
                            if isinstance(parsed[0], dict):
                                has_segment_fields = any(
                                    key in parsed[0] for key in ['script', 'name', 'start_time', 'text']
                                )
                                if has_segment_fields:
                                    # Calculate total script content length
                                    total_script_len = sum(
                                        len(item.get('script', '') or item.get('text', ''))
                                        for item in parsed if isinstance(item, dict)
                                    )
                                    # Check if this looks like placeholder/example content
                                    # Skip if scripts contain obvious placeholder text
                                    scripts_text = ' '.join(
                                        (item.get('script', '') or item.get('text', ''))
                                        for item in parsed if isinstance(item, dict)
                                    ).lower()
                                    is_placeholder = any(phrase in scripts_text for phrase in [
                                        'your narration',
                                        'narration text here',
                                        'script here',
                                        'your script here',
                                        '[your',
                                        'placeholder',
                                    ])
                                    if is_placeholder:
                                        logger.debug(f"Skipping placeholder JSON array with {len(parsed)} segments")
                                        start_pos = bracket_start + 1
                                        continue
                                    candidates.append((candidate, parsed, total_script_len))
                                    logger.debug(f"Found candidate JSON array with {len(parsed)} segments, {total_script_len} chars of script content")
                    except json.JSONDecodeError:
                        pass

                start_pos = bracket_start + 1

            if not candidates:
                return None

            # Return the candidate with the most script content
            # This avoids returning placeholder/example JSON from the prompt
            best_candidate = max(candidates, key=lambda x: x[2])
            logger.info(f"Selected best JSON array: {len(best_candidate[1])} segments, {best_candidate[2]} chars of script content (from {len(candidates)} candidates)")
            return best_candidate[0]

        # Strategy 1: Try to find JSON array directly in full response
        result = try_extract_json_array(response)
        if result:
            return result

        # Strategy 2: Extract content after thinking blocks first, then try JSON
        # Model outputs: <think>...</think> followed by actual content
        # Find the LAST </think> tag and take everything after it
        think_end_matches = list(re.finditer(r'</think>\s*', response, flags=re.IGNORECASE))
        if think_end_matches:
            last_think_end = think_end_matches[-1]
            after_think = response[last_think_end.end():].strip()
            logger.info(f"Extracted {len(after_think)} chars after last </think>")

            # Try to find JSON in the content after </think>
            result = try_extract_json_array(after_think)
            if result:
                return result

            # If no valid JSON found, use the content after </think> as cleaned text
            cleaned = after_think
        else:
            cleaned = response

        # Strategy 3: Remove any remaining thinking artifacts
        # Remove complete <think>...</think> blocks
        cleaned = re.sub(r'<think>[\s\S]*?</think>', '', cleaned, flags=re.IGNORECASE)
        # Remove incomplete opening <think> tag and everything after
        cleaned = re.sub(r'<think>[\s\S]*$', '', cleaned, flags=re.IGNORECASE)
        # Remove content before incomplete closing </think>
        cleaned = re.sub(r'^[\s\S]*?</think>', '', cleaned, flags=re.IGNORECASE)
        # Remove stray think tags
        cleaned = re.sub(r'</?think\s*/?>', '', cleaned, flags=re.IGNORECASE)

        # Remove common artifacts
        cleaned = re.sub(r'<\|.*?\|>', '', cleaned)  # Special tokens like <|endoftext|>
        cleaned = re.sub(r'```json\s*', '', cleaned)  # Markdown JSON blocks
        cleaned = re.sub(r'```\s*', '', cleaned)      # Markdown code blocks

        result = cleaned.strip()
        logger.info(f"Cleaned result ({len(result)} chars): {result[:300] if result else '(empty)'}...")
        return result

    async def generate_single_segment_script(
        self,
        provider_config: LLMProvider,
        segment_spec: SegmentSpec,
        video_context: VideoContext,
        style: StyleProfile,
        user_context: str
    ) -> Dict[str, Any]:
        """Generate a script for a single segment based on user context"""

        prompt = self.prompt_engineer.build_single_segment_suggestion_prompt(
            segment_spec=segment_spec,
            video_context=video_context,
            style=style,
            user_context=user_context
        )

        provider = self.llm_service.get_provider(provider_config)
        response = await provider.generate(prompt, max_tokens=300, temperature=0.7)

        script = response.strip()
        analysis = self.script_analyzer.analyze_segment(
            script,
            segment_spec.start_time,
            segment_spec.end_time,
            segment_spec.index
        )

        return {
            "script": script,
            "word_count": analysis.word_count,
            "estimated_duration": analysis.estimated_duration,
            "fits_duration": analysis.fit_status in [FitStatus.PERFECT, FitStatus.GOOD, FitStatus.ACCEPTABLE],
            "fit_percentage": analysis.fit_percentage,
            "suggestions": self.script_analyzer.get_refinement_suggestions(analysis)
        }


# Singleton instance
intelligent_generator = IntelligentScriptGenerator(
    prompt_engineer=prompt_engineer,
    script_analyzer=script_analyzer,
    llm_service=llm_service
)
