"""
LLM Service - Unified interface for multiple LLM providers using LangChain
============================================================================

Supports:
- Ollama (local, free) with structured outputs
- OpenAI (GPT-4, GPT-4o, GPT-3.5)
- Anthropic (Claude 3, Claude 3.5)
- Azure OpenAI (GPT models on Azure)
- Google Gemini (Gemini Pro, Gemini Flash)
- AWS Bedrock (Claude, Llama, Titan, etc.)
- HuggingFace (Various open-source models)
- Custom endpoints (OpenAI-compatible)

LangChain Integration:
This module uses LangChain for unified LLM provider access. LangChain provides:
- Consistent interface across all providers
- Native structured output support
- Streaming capabilities
- Tool/function calling
- Automatic retries and error handling

Structured Outputs:
All providers support structured outputs via LangChain's with_structured_output()
or native JSON schema support (Ollama format parameter).
Reference: https://python.langchain.com/docs/

Video Understanding:
Supports video analysis using vision-language models (VLMs) like qwen3-vl.
Videos are processed by extracting key frames and sending them as images.
The Chat API with images in message provides best results.
"""

import asyncio
import base64
import json
import re
import subprocess
import tempfile
import os
from typing import Optional, List, Dict, Tuple, Any, Type
from abc import ABC, abstractmethod
from pydantic import BaseModel
from pathlib import Path

import httpx

from web_ui.api.schemas.llm_schemas import (
    LLMProvider,
    SegmentDescription,
    ScriptStyle,
    GeneratedScript,
    OllamaModel,
    AutoGeneratedSegment,
    ExistingSegmentContext,
    LLMScriptGenerationOutput,
    LLMAutoSegmentOutput,
)
from utils.logger import logger

# Import LangChain providers
try:
    from web_ui.api.services.langchain_providers import (
        ProviderConfig,
        LangChainProviderFactory,
        BaseLangChainProvider,
        OllamaLangChainProvider,
        OpenAILangChainProvider,
        AnthropicLangChainProvider,
        AzureOpenAILangChainProvider,
        GoogleGeminiLangChainProvider,
        AWSBedrockLangChainProvider,
        HuggingFaceLangChainProvider,
        CustomLangChainProvider,
    )
    LANGCHAIN_AVAILABLE = True
    logger.info("[LLM Service] LangChain providers loaded successfully")
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    logger.warning(f"[LLM Service] LangChain providers not available: {e}")
    logger.warning("[LLM Service] Falling back to httpx-based providers")


# ============================================================================
# Pydantic models for structured outputs
# ============================================================================

class VideoSegment(BaseModel):
    """Single video segment with script"""
    name: str
    start_time: float
    end_time: float
    description: str
    script: str


class VideoSegmentList(BaseModel):
    """List of video segments - used for structured output"""
    segments: List[VideoSegment]


class BaseLLMProvider(ABC):
    """Base class for LLM providers"""

    def __init__(self, api_key: Optional[str] = None, endpoint: Optional[str] = None):
        self.api_key = api_key
        self.endpoint = endpoint

    @abstractmethod
    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text from prompt"""
        pass

    @abstractmethod
    async def is_available(self) -> bool:
        """Check if provider is available"""
        pass


class OllamaProvider(BaseLLMProvider):
    """
    Ollama local LLM provider

    Supports the Ollama API including:
    - /api/generate for text generation
    - /api/chat for chat-based generation
    - think parameter for controlling thinking mode in reasoning models

    Reference: https://github.com/ollama/ollama/blob/main/docs/api.md
    """

    DEFAULT_ENDPOINT = "http://localhost:11434"

    # Models known to support thinking mode
    THINKING_MODELS = ['qwen3', 'deepseek-r1', 'qwq']

    def __init__(self, model: str, endpoint: Optional[str] = None, **kwargs):
        super().__init__(endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model
        # Check if this model supports thinking mode
        self._supports_thinking = any(m in self.model.lower() for m in self.THINKING_MODELS)

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        think: Optional[bool] = None,
        system: Optional[str] = None
    ) -> str:
        """
        Generate text using Ollama API

        Args:
            prompt: The input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0 - 1.0)
            think: Enable/disable thinking mode for reasoning models.
                   None = auto (disable for JSON generation tasks)
                   True = enable thinking (returns thinking + response separately)
                   False = disable thinking (direct response only)
            system: Optional system prompt

        Returns:
            Generated text (content only, thinking is separated if enabled)
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        # Build request payload
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Add system prompt if provided
        if system:
            payload["system"] = system

        # Handle thinking mode for supported models
        # Per Ollama docs: think=false disables thinking, response comes directly
        if self._supports_thinking:
            if think is None:
                # Default: disable thinking for cleaner JSON output
                payload["think"] = False
                logger.info(f"Auto-disabled thinking for model: {self.model}")
            else:
                payload["think"] = think
                logger.info(f"Thinking mode {'enabled' if think else 'disabled'} for: {self.model}")

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                logger.info(f"Calling Ollama model: {self.model} with {len(prompt)} char prompt")
                response = await client.post(
                    f"{self.endpoint}/api/generate",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                result = data.get("response", "")

                # Log if thinking was present (for debugging)
                if "thinking" in data:
                    logger.debug(f"Model returned thinking content ({len(data.get('thinking', ''))} chars)")

                logger.info(f"Ollama generation successful: {len(result)} chars")
                return result

            except httpx.TimeoutException as e:
                error_msg = f"Ollama request timed out after 300s: {str(e) or 'No additional details'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except httpx.HTTPStatusError as e:
                error_msg = f"Ollama HTTP error {e.response.status_code}: {e.response.text[:200] if e.response.text else 'No response body'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except Exception as e:
                error_msg = f"Ollama generation failed: {type(e).__name__}: {str(e) or 'Unknown error'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e

    async def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        think: Optional[bool] = None
    ) -> str:
        """
        Chat-based generation using Ollama /api/chat endpoint

        Args:
            messages: List of message dicts with 'role' and 'content' keys
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            think: Enable/disable thinking mode

        Returns:
            Assistant's response content
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Handle thinking mode
        if self._supports_thinking:
            payload["think"] = think if think is not None else False

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                message = data.get("message", {})
                return message.get("content", "")

            except Exception as e:
                logger.error(f"Ollama chat error: {e}")
                raise

    async def generate_structured(
        self,
        prompt: str,
        response_schema: Type[BaseModel],
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system: Optional[str] = None
    ) -> BaseModel:
        """
        Generate structured output using Ollama's format parameter.

        This forces the model to return valid JSON matching the provided schema,
        eliminating parsing issues with thinking models like qwen3.

        Reference: https://ollama.com/blog/structured-outputs

        Args:
            prompt: The input prompt
            response_schema: A Pydantic model class defining the expected response structure
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (lower = more deterministic)
            system: Optional system prompt

        Returns:
            Validated Pydantic model instance
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        # Get JSON schema from Pydantic model
        json_schema = response_schema.model_json_schema()

        # Build messages for chat endpoint
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "format": json_schema,  # This is the key: structured output format
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Note: When using format parameter, thinking mode is automatically handled
        # The model will output structured JSON directly

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                logger.info(f"Calling Ollama with structured output for model: {self.model}")
                logger.debug(f"Schema: {json_schema}")

                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                content = data.get("message", {}).get("content", "")

                logger.info(f"Ollama structured generation successful: {len(content)} chars")
                logger.debug(f"Raw content: {content[:500]}...")

                # Parse and validate the response using Pydantic
                result = response_schema.model_validate_json(content)
                return result

            except httpx.TimeoutException as e:
                error_msg = f"Ollama request timed out: {str(e) or 'No details'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except httpx.HTTPStatusError as e:
                error_msg = f"Ollama HTTP error {e.response.status_code}: {e.response.text[:200] if e.response.text else 'No body'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except json.JSONDecodeError as e:
                error_msg = f"Failed to parse Ollama response as JSON: {e}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except Exception as e:
                error_msg = f"Ollama structured generation failed: {type(e).__name__}: {str(e) or 'Unknown error'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e

    async def is_available(self) -> bool:
        """Check if Ollama is running"""
        async with httpx.AsyncClient(timeout=5.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/api/tags")
                return response.status_code == 200
            except:
                return False

    async def list_models(self) -> List[OllamaModel]:
        """List available Ollama models"""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/api/tags")
                response.raise_for_status()
                data = response.json()
                models = []
                for model in data.get("models", []):
                    models.append(OllamaModel(
                        name=model.get("name", "unknown"),
                        size=self._format_size(model.get("size", 0)),
                        modified_at=model.get("modified_at", ""),
                        digest=model.get("digest", "")[:12]
                    ))
                return models
            except Exception as e:
                logger.error(f"Failed to list Ollama models: {e}")
                return []

    def _format_size(self, size_bytes: int) -> str:
        """Format bytes to human readable"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"

    # =========================================================================
    # Video Understanding Methods (for VLMs like qwen3-vl)
    # =========================================================================

    async def extract_video_frames(
        self,
        video_path: str,
        num_frames: int = 4,
        max_dimension: int = 1280
    ) -> Tuple[List[str], float]:
        """
        Extract key frames from a video for analysis.

        Args:
            video_path: Path to the video file
            num_frames: Number of frames to extract (evenly distributed)
            max_dimension: Maximum width/height for frames (for efficiency)

        Returns:
            Tuple of (list of base64-encoded frames, video duration in seconds)
        """
        # Get video duration using ffprobe
        probe_cmd = [
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_format', '-show_streams', video_path
        ]
        result = subprocess.run(probe_cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"ffprobe failed: {result.stderr}")

        probe_data = json.loads(result.stdout)
        duration = float(probe_data['format']['duration'])

        # Calculate frame extraction timestamps
        interval = duration / (num_frames + 1)
        frames_base64 = []

        with tempfile.TemporaryDirectory() as tmpdir:
            for i in range(num_frames):
                timestamp = interval * (i + 1)
                frame_path = os.path.join(tmpdir, f"frame_{i}.jpg")

                # Extract frame using ffmpeg with scaling
                cmd = [
                    'ffmpeg', '-y', '-ss', str(timestamp), '-i', video_path,
                    '-vframes', '1',
                    '-vf', f'scale=min({max_dimension}\\,iw):min({max_dimension}\\,ih):force_original_aspect_ratio=decrease',
                    '-q:v', '2', frame_path
                ]
                subprocess.run(cmd, capture_output=True)

                if os.path.exists(frame_path):
                    with open(frame_path, 'rb') as f:
                        frame_base64 = base64.b64encode(f.read()).decode('utf-8')
                        frames_base64.append(frame_base64)
                        logger.debug(f"Extracted frame {i+1} at {timestamp:.2f}s")

        logger.info(f"Extracted {len(frames_base64)} frames from {duration:.2f}s video")
        return frames_base64, duration

    async def analyze_video(
        self,
        video_path: str,
        prompt: Optional[str] = None,
        num_frames: int = 4,
        vision_model: str = "qwen3-vl:8b"
    ) -> Dict[str, Any]:
        """
        Analyze a video using a vision-language model.

        Extracts key frames from the video and sends them to a VLM for analysis.
        Best used with models like qwen3-vl that support image understanding.

        Args:
            video_path: Path to the video file
            prompt: Custom prompt for analysis. If None, uses default description prompt.
            num_frames: Number of frames to extract (default: 4)
            vision_model: The vision model to use (default: qwen3-vl:8b)

        Returns:
            Dict with keys:
                - description: The video description/analysis
                - duration: Video duration in seconds
                - frames_analyzed: Number of frames sent to model
                - success: Whether analysis succeeded
                - error: Error message if failed
        """
        result = {
            "description": "",
            "duration": 0.0,
            "frames_analyzed": 0,
            "success": False,
            "error": None
        }

        try:
            # Extract frames
            frames_base64, duration = await self.extract_video_frames(
                video_path, num_frames=num_frames
            )
            result["duration"] = duration
            result["frames_analyzed"] = len(frames_base64)

            if not frames_base64:
                result["error"] = "Failed to extract frames from video"
                return result

            # Default prompt for video analysis
            if prompt is None:
                prompt = f"""Analyze these {len(frames_base64)} frames extracted from a video and provide:
1. A detailed description of what the video shows
2. The main subject or topic
3. Key visual elements, text, or objects visible
4. The overall mood or style of the video
5. Any actions or movements occurring

Provide a comprehensive description that would help understand this video without watching it."""

            # Build chat request with images
            payload = {
                "model": vision_model,
                "messages": [{
                    "role": "user",
                    "content": prompt,
                    "images": frames_base64
                }],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 2000,
                }
            }

            timeout = httpx.Timeout(180.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                logger.info(f"Sending {len(frames_base64)} frames to {vision_model} for analysis")

                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                message = data.get("message", {})

                # Get content, fallback to thinking if content is empty
                content = message.get("content", "")
                thinking = message.get("thinking", "")

                # Use content if available, otherwise use thinking
                description = content if content else thinking

                # Clean up the description
                description = self._clean_video_description(description)

                if description:
                    result["description"] = description
                    result["success"] = True
                    logger.info(f"Video analysis successful: {len(description)} chars")
                else:
                    result["error"] = "Model returned empty response"
                    logger.warning("Video analysis returned empty response")

        except subprocess.SubprocessError as e:
            result["error"] = f"Video processing error: {str(e)}"
            logger.error(f"Video frame extraction failed: {e}")
        except httpx.TimeoutException:
            result["error"] = "Analysis timed out"
            logger.error("Video analysis request timed out")
        except httpx.HTTPStatusError as e:
            result["error"] = f"API error: {e.response.status_code}"
            logger.error(f"Video analysis HTTP error: {e}")
        except Exception as e:
            result["error"] = f"Analysis failed: {str(e)}"
            logger.error(f"Video analysis error: {type(e).__name__}: {e}")

        return result

    def _clean_video_description(self, description: str) -> str:
        """Clean up video description from model output"""
        if not description:
            return ""

        # Remove thinking tags if present
        description = re.sub(r'<think>[\s\S]*?</think>', '', description, flags=re.IGNORECASE)
        description = re.sub(r'<\|.*?\|>', '', description)  # Remove special tokens

        # Remove meta-commentary and thinking patterns
        # These patterns indicate the model is thinking about the task rather than describing
        meta_patterns = [
            r"(Got it|Okay|OK|Alright),?\s*(let's|let me|I'll|I will)\s+[^.]*\.\s*",
            r"(First|Starting|Beginning),?\s+I\s+(need to|will|should)\s+[^.]*\.\s*",
            r"Wait,?\s+[^.]*\.\s*",
            r"Looking at\s+(the\s+)?(user's|this|these)\s+[^.]*,?\s*",
            r"The user('s)?\s+(provided|says|input|message)[^.]*\.\s*",
            r"\[img\]",  # Image placeholder tokens
        ]
        for pattern in meta_patterns:
            description = re.sub(pattern, '', description, flags=re.IGNORECASE)

        # Clean up common prefixes from thinking mode
        thinking_prefixes = [
            r"^(So,?\s+)?let('s|me)\s+(look|analyze|describe|examine|break)",
            r"^(Okay|OK),?\s+(so\s+)?(the\s+)?(user|image|video)",
            r"^(Hmm|Well|Now),?\s+",
        ]
        for prefix in thinking_prefixes:
            description = re.sub(prefix, '', description, flags=re.IGNORECASE)

        # Remove multiple consecutive newlines
        description = re.sub(r'\n{3,}', '\n\n', description)

        return description.strip()


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API provider"""

    DEFAULT_ENDPOINT = "https://api.openai.com/v1"

    def __init__(self, api_key: str, model: str = "gpt-4-turbo-preview", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using OpenAI API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"OpenAI generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if OpenAI API is accessible"""
        if not self.api_key:
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(
                    f"{self.endpoint}/models",
                    headers={"Authorization": f"Bearer {self.api_key}"}
                )
                return response.status_code == 200
            except:
                return False


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude API provider"""

    DEFAULT_ENDPOINT = "https://api.anthropic.com/v1"

    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using Anthropic API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/messages",
                    headers={
                        "x-api-key": self.api_key,
                        "Content-Type": "application/json",
                        "anthropic-version": "2023-06-01"
                    },
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["content"][0]["text"]
            except Exception as e:
                logger.error(f"Anthropic generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if Anthropic API is accessible"""
        return bool(self.api_key)


class GoogleProvider(BaseLLMProvider):
    """Google Gemini API provider"""

    DEFAULT_ENDPOINT = "https://generativelanguage.googleapis.com/v1beta"

    def __init__(self, api_key: str, model: str = "gemini-pro", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using Google Gemini API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/models/{self.model}:generateContent",
                    params={"key": self.api_key},
                    json={
                        "contents": [{"parts": [{"text": prompt}]}],
                        "generationConfig": {
                            "maxOutputTokens": max_tokens,
                            "temperature": temperature
                        }
                    }
                )
                response.raise_for_status()
                return response.json()["candidates"][0]["content"]["parts"][0]["text"]
            except Exception as e:
                logger.error(f"Google generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if Google API is accessible"""
        return bool(self.api_key)


class CustomProvider(BaseLLMProvider):
    """Custom LLM endpoint provider (OpenAI-compatible)"""

    def __init__(self, api_key: Optional[str], endpoint: str, model: str = "default", **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using custom endpoint (OpenAI-compatible format)"""
        async with httpx.AsyncClient(timeout=120.0) as client:
            headers = {"Content-Type": "application/json"}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            try:
                response = await client.post(
                    f"{self.endpoint}/chat/completions",
                    headers=headers,
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"Custom provider generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if custom endpoint is accessible"""
        if not self.endpoint:
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/models")
                return response.status_code in [200, 401]  # 401 means endpoint exists but needs auth
            except:
                return False


class LLMService:
    """
    Unified LLM Service for script generation using LangChain

    Handles:
    - Multiple provider support via LangChain
    - Smart prompt engineering for video scripts
    - Duration-aware script generation
    - Script refinement and adjustment
    - Structured output generation

    Supported Providers:
    - ollama: Local models via Ollama
    - openai: OpenAI GPT models
    - anthropic: Anthropic Claude models
    - azure_openai: OpenAI models on Azure
    - google: Google Gemini models
    - aws_bedrock: AWS Bedrock models
    - huggingface: HuggingFace Hub models
    - custom: Any OpenAI-compatible endpoint
    """

    # Legacy provider map for backward compatibility
    PROVIDERS = {
        "ollama": OllamaProvider,
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
        "google": GoogleProvider,
        "custom": CustomProvider,
    }

    # LangChain provider map
    LANGCHAIN_PROVIDERS = {
        "ollama": OllamaLangChainProvider if LANGCHAIN_AVAILABLE else None,
        "openai": OpenAILangChainProvider if LANGCHAIN_AVAILABLE else None,
        "anthropic": AnthropicLangChainProvider if LANGCHAIN_AVAILABLE else None,
        "azure_openai": AzureOpenAILangChainProvider if LANGCHAIN_AVAILABLE else None,
        "google": GoogleGeminiLangChainProvider if LANGCHAIN_AVAILABLE else None,
        "aws_bedrock": AWSBedrockLangChainProvider if LANGCHAIN_AVAILABLE else None,
        "huggingface": HuggingFaceLangChainProvider if LANGCHAIN_AVAILABLE else None,
        "custom": CustomLangChainProvider if LANGCHAIN_AVAILABLE else None,
    }

    # Average words per second for duration estimation (adjustable per language)
    WORDS_PER_SECOND = {
        "en": 2.5,
        "es": 2.3,
        "fr": 2.4,
        "de": 2.2,
        "it": 2.4,
        "pt": 2.3,
        "hi": 2.8,
        "zh": 3.5,  # Characters per second for Chinese
        "ja": 3.0,
        "ko": 2.8,
    }

    def __init__(self):
        self.ollama_provider = OllamaProvider(model="llama3.2:3b")
        self._langchain_providers_cache = {}
        logger.info(f"[LLM Service] Initialized (LangChain available: {LANGCHAIN_AVAILABLE})")

    def _merge_with_saved_settings(self, config: LLMProvider) -> LLMProvider:
        """
        Merge user-provided config with saved LLM settings.

        If the user hasn't provided credentials (api_key, etc.), use the saved settings.
        This allows users to configure credentials once in Settings and use them everywhere.
        """
        try:
            # Import saved settings
            from web_ui.api.routes.settings_routes import llm_settings

            # Create a copy of the config dict to modify
            config_dict = config.model_dump()

            provider_type = config.type

            # Only merge if user hasn't provided the credential
            if provider_type == "openai" and not config.api_key:
                if llm_settings.openai_api_key:
                    config_dict["api_key"] = llm_settings.openai_api_key
                    logger.debug("[LLM Service] Using saved OpenAI API key")

            elif provider_type == "anthropic" and not config.api_key:
                if llm_settings.anthropic_api_key:
                    config_dict["api_key"] = llm_settings.anthropic_api_key
                    logger.debug("[LLM Service] Using saved Anthropic API key")

            elif provider_type == "google" and not config.api_key:
                if llm_settings.google_api_key:
                    config_dict["api_key"] = llm_settings.google_api_key
                    logger.debug("[LLM Service] Using saved Google API key")

            elif provider_type == "huggingface":
                if not config.api_key and llm_settings.huggingface_api_key:
                    config_dict["api_key"] = llm_settings.huggingface_api_key
                    logger.debug("[LLM Service] Using saved HuggingFace API key")
                if not config.huggingface_provider and llm_settings.huggingface_inference_provider:
                    config_dict["huggingface_provider"] = llm_settings.huggingface_inference_provider

            elif provider_type == "azure_openai":
                if not config.api_key and llm_settings.azure_openai_api_key:
                    config_dict["api_key"] = llm_settings.azure_openai_api_key
                    logger.debug("[LLM Service] Using saved Azure OpenAI API key")
                if not config.endpoint and llm_settings.azure_openai_endpoint:
                    config_dict["endpoint"] = llm_settings.azure_openai_endpoint
                if not config.azure_deployment and llm_settings.azure_openai_deployment:
                    config_dict["azure_deployment"] = llm_settings.azure_openai_deployment
                if not config.azure_api_version and llm_settings.azure_openai_api_version:
                    config_dict["azure_api_version"] = llm_settings.azure_openai_api_version

            elif provider_type == "aws_bedrock":
                if not config.aws_access_key_id and llm_settings.aws_access_key_id:
                    config_dict["aws_access_key_id"] = llm_settings.aws_access_key_id
                    logger.debug("[LLM Service] Using saved AWS access key")
                if not config.aws_secret_access_key and llm_settings.aws_secret_access_key:
                    config_dict["aws_secret_access_key"] = llm_settings.aws_secret_access_key
                if not config.aws_region and llm_settings.aws_region:
                    config_dict["aws_region"] = llm_settings.aws_region

            elif provider_type == "custom":
                if not config.api_key and llm_settings.custom_api_key:
                    config_dict["api_key"] = llm_settings.custom_api_key
                    logger.debug("[LLM Service] Using saved custom API key")
                if not config.endpoint and llm_settings.custom_endpoint:
                    config_dict["endpoint"] = llm_settings.custom_endpoint

            elif provider_type == "ollama":
                if not config.endpoint and llm_settings.ollama_endpoint:
                    config_dict["endpoint"] = llm_settings.ollama_endpoint

            # Return new LLMProvider with merged settings
            return LLMProvider(**config_dict)

        except Exception as e:
            logger.warning(f"[LLM Service] Failed to merge saved settings: {e}")
            return config

    def get_provider(self, config: LLMProvider) -> BaseLLMProvider:
        """
        Get provider instance from config.

        Attempts to use LangChain providers first for better integration,
        falls back to legacy httpx-based providers if LangChain is unavailable.

        Automatically merges with saved LLM settings for missing credentials.
        """
        # Merge with saved settings to fill in missing credentials
        config = self._merge_with_saved_settings(config)
        provider_type = config.type

        # Try LangChain provider first
        if LANGCHAIN_AVAILABLE and provider_type in self.LANGCHAIN_PROVIDERS:
            return self._get_langchain_provider(config)

        # Fallback to legacy providers
        provider_class = self.PROVIDERS.get(provider_type)
        if not provider_class:
            supported = list(self.PROVIDERS.keys())
            if LANGCHAIN_AVAILABLE:
                supported.extend(["azure_openai", "aws_bedrock", "huggingface"])
            raise ValueError(f"Unknown provider type: {provider_type}. Supported: {supported}")

        logger.info(f"[LLM Service] Using legacy {provider_type} provider")
        return provider_class(
            api_key=config.api_key,
            endpoint=config.endpoint,
            model=config.model
        )

    def _get_langchain_provider(self, config: LLMProvider) -> BaseLangChainProvider:
        """Get or create a LangChain provider instance."""
        # Create cache key based on provider config
        cache_key_parts = [config.type, config.model, config.endpoint or 'default']

        # Add Azure-specific parts to cache key
        if config.type == "azure_openai":
            cache_key_parts.append(config.azure_deployment or 'default')

        cache_key = ":".join(cache_key_parts)

        if cache_key in self._langchain_providers_cache:
            logger.debug(f"[LLM Service] Using cached LangChain {config.type} provider")
            return self._langchain_providers_cache[cache_key]

        # Create ProviderConfig from LLMProvider with all provider-specific fields
        provider_config_dict = {
            "type": config.type,
            "model": config.model,
            "api_key": config.api_key,
            "endpoint": config.endpoint,
        }

        # Add Azure OpenAI specific fields
        if config.type == "azure_openai":
            provider_config_dict.update({
                "azure_deployment": config.azure_deployment,
                "azure_api_version": config.azure_api_version,
            })

        # Add AWS Bedrock specific fields
        if config.type == "aws_bedrock":
            provider_config_dict.update({
                "aws_region": config.aws_region,
                "aws_access_key_id": config.aws_access_key_id,
                "aws_secret_access_key": config.aws_secret_access_key,
            })

        # Add HuggingFace specific fields
        if config.type == "huggingface":
            provider_config_dict.update({
                "huggingface_provider": config.huggingface_provider,
            })

        provider_config = ProviderConfig(**provider_config_dict)

        # Create provider using factory
        provider = LangChainProviderFactory.create(provider_config)

        # Cache the provider
        self._langchain_providers_cache[cache_key] = provider

        logger.info(f"[LLM Service] Created LangChain {config.type} provider for model: {config.model}")
        return provider

    def get_langchain_provider(self, config: LLMProvider) -> BaseLangChainProvider:
        """
        Get a LangChain provider directly.

        This is useful when you need access to LangChain-specific features
        like structured output, streaming, or tool calling.
        """
        if not LANGCHAIN_AVAILABLE:
            raise RuntimeError("LangChain providers are not available. Install langchain packages.")
        return self._get_langchain_provider(config)

    def estimate_duration(self, text: str, language: str = "en") -> float:
        """Estimate audio duration for text"""
        wps = self.WORDS_PER_SECOND.get(language, 2.5)
        word_count = len(text.split())
        return word_count / wps

    def estimate_word_count(self, duration: float, language: str = "en") -> int:
        """Estimate word count for target duration"""
        wps = self.WORDS_PER_SECOND.get(language, 2.5)
        return int(duration * wps)

    def _get_system_prompt(self, style: ScriptStyle) -> str:
        """
        Build a comprehensive system prompt for voice-over script generation.

        This prompt is designed to work with any LLM provider (OpenAI, Anthropic,
        Google, Ollama, etc.) and ensures consistent, high-quality output.
        """
        # Language-specific guidance
        language_name = self._get_language_name(style.language)

        return f"""You are an expert voice-over scriptwriter. Your task is to write narration scripts for video content.

CRITICAL RULES - YOU MUST FOLLOW THESE:

1. OUTPUT LANGUAGE: Write ALL scripts in {language_name} ({style.language}).

2. STICK TO THE DESCRIPTION:
   - Only write about what the user describes
   - Do NOT invent details, facts, or information not provided
   - Do NOT assume what the video shows unless explicitly told
   - If the description is vague, write generic but relevant narration

3. NO META-COMMENTARY:
   - Do NOT explain what you're doing
   - Do NOT say "I will write..." or "Here is the script..."
   - Do NOT include thinking or reasoning
   - Output ONLY the requested JSON format

4. NO PLACEHOLDERS:
   - Never write "..." or "[your text here]" or "insert text"
   - Every script must be complete, real narration text
   - Write actual sentences that can be spoken aloud

5. WORD COUNT IS CRITICAL:
   - Each script must match the specified word count (¬±10%)
   - This ensures the narration fits the video timing
   - Count words carefully before outputting

6. STYLE REQUIREMENTS:
   - Tone: {style.tone}
   - Style: {style.style}
   - Target audience: {style.audience}

7. OUTPUT FORMAT:
   - Return ONLY a valid JSON array
   - No markdown code blocks, no explanations
   - Start directly with [ and end with ]"""

    # Comprehensive language map covering world languages
    # ISO 639-1 codes mapped to full names with native script where applicable
    LANGUAGE_MAP = {
        # Major World Languages
        "en": "English",
        "zh": "Chinese (‰∏≠Êñá)",
        "zh-CN": "Simplified Chinese (ÁÆÄ‰Ωì‰∏≠Êñá)",
        "zh-TW": "Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá)",
        "es": "Spanish (Espa√±ol)",
        "ar": "Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)",
        "hi": "Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)",
        "bn": "Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)",
        "pt": "Portuguese (Portugu√™s)",
        "pt-BR": "Brazilian Portuguese (Portugu√™s Brasileiro)",
        "ru": "Russian (–†—É—Å—Å–∫–∏–π)",
        "ja": "Japanese (Êó•Êú¨Ë™û)",
        "pa": "Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)",
        "de": "German (Deutsch)",
        "jv": "Javanese (Basa Jawa)",
        "ko": "Korean (ÌïúÍµ≠Ïñ¥)",
        "fr": "French (Fran√ßais)",
        "te": "Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)",
        "mr": "Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)",
        "tr": "Turkish (T√ºrk√ße)",
        "ta": "Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)",
        "vi": "Vietnamese (Ti·∫øng Vi·ªát)",
        "ur": "Urdu (ÿßÿ±ÿØŸà)",
        "it": "Italian (Italiano)",
        "th": "Thai (‡πÑ‡∏ó‡∏¢)",
        "gu": "Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)",
        "fa": "Persian/Farsi (ŸÅÿßÿ±ÿ≥€å)",
        "pl": "Polish (Polski)",
        "uk": "Ukrainian (–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞)",
        "ml": "Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)",
        "kn": "Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)",
        "or": "Odia/Oriya (‡¨ì‡¨°‡¨º‡¨ø‡¨Ü)",
        "my": "Burmese (·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨)",
        "su": "Sundanese (Basa Sunda)",
        "ro": "Romanian (Rom√¢nƒÉ)",
        "nl": "Dutch (Nederlands)",
        "hu": "Hungarian (Magyar)",
        "el": "Greek (ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨)",
        "cs": "Czech (ƒåe≈°tina)",
        "sv": "Swedish (Svenska)",
        "he": "Hebrew (◊¢◊ë◊®◊ô◊™)",
        "be": "Belarusian (–ë–µ–ª–∞—Ä—É—Å–∫–∞—è)",
        "bg": "Bulgarian (–ë—ä–ª–≥–∞—Ä—Å–∫–∏)",
        "az": "Azerbaijani (Az…ôrbaycan)",
        "sr": "Serbian (–°—Ä–ø—Å–∫–∏)",
        "hr": "Croatian (Hrvatski)",
        "sk": "Slovak (Slovenƒçina)",
        "da": "Danish (Dansk)",
        "fi": "Finnish (Suomi)",
        "no": "Norwegian (Norsk)",
        "nb": "Norwegian Bokm√•l (Norsk Bokm√•l)",
        "nn": "Norwegian Nynorsk (Norsk Nynorsk)",
        "ca": "Catalan (Catal√†)",
        "lt": "Lithuanian (Lietuvi≈≥)",
        "lv": "Latvian (Latvie≈°u)",
        "et": "Estonian (Eesti)",
        "sl": "Slovenian (Sloven≈°ƒçina)",
        "mk": "Macedonian (–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏)",
        "sq": "Albanian (Shqip)",
        "bs": "Bosnian (Bosanski)",
        "is": "Icelandic (√çslenska)",
        "ga": "Irish (Gaeilge)",
        "cy": "Welsh (Cymraeg)",
        "gd": "Scottish Gaelic (G√†idhlig)",
        "mt": "Maltese (Malti)",
        "lb": "Luxembourgish (L√´tzebuergesch)",
        "eu": "Basque (Euskara)",
        "gl": "Galician (Galego)",

        # South Asian Languages
        "as": "Assamese (‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ)",
        "ne": "Nepali (‡§®‡•á‡§™‡§æ‡§≤‡•Ä)",
        "si": "Sinhala (‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω)",
        "sd": "Sindhi (ÿ≥ŸÜ⁄åŸä)",
        "ks": "Kashmiri (‡§ï‡•â‡§∂‡•Å‡§∞)",
        "sa": "Sanskrit (‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç)",
        "bh": "Bihari (‡§≠‡•ã‡§ú‡§™‡•Å‡§∞‡•Ä)",
        "doi": "Dogri (‡§°‡•ã‡§ó‡§∞‡•Ä)",
        "kok": "Konkani (‡§ï‡•ã‡§Ç‡§ï‡§£‡•Ä)",
        "mai": "Maithili (‡§Æ‡•à‡§•‡§ø‡§≤‡•Ä)",
        "mni": "Manipuri (‡¶Æ‡¶£‡¶ø‡¶™‡ßÅ‡¶∞‡ßÄ)",
        "sat": "Santali (·±•·±ü·±±·±õ·±ü·±≤·±§)",

        # Southeast Asian Languages
        "id": "Indonesian (Bahasa Indonesia)",
        "ms": "Malay (Bahasa Melayu)",
        "tl": "Tagalog (Tagalog)",
        "fil": "Filipino (Filipino)",
        "ceb": "Cebuano (Cebuano)",
        "ilo": "Ilocano (Ilokano)",
        "km": "Khmer (·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö)",
        "lo": "Lao (‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß)",
        "hmn": "Hmong (Hmoob)",

        # East Asian Languages
        "yue": "Cantonese (Á≤µË™û)",
        "nan": "Min Nan/Hokkien (Èñ©ÂçóË™û)",
        "hak": "Hakka (ÂÆ¢ÂÆ∂Ë©±)",
        "mn": "Mongolian (–ú–æ–Ω–≥–æ–ª)",

        # Central Asian Languages
        "kk": "Kazakh (“ö–∞–∑–∞“õ—à–∞)",
        "uz": "Uzbek (O'zbek)",
        "ky": "Kyrgyz (–ö—ã—Ä–≥—ã–∑—á–∞)",
        "tg": "Tajik (–¢–æ“∑–∏–∫”£)",
        "tk": "Turkmen (T√ºrkmen)",

        # Middle Eastern Languages
        "ku": "Kurdish (Kurd√Æ)",
        "ckb": "Central Kurdish/Sorani (ÿ≥€Üÿ±ÿßŸÜ€å)",
        "kmr": "Northern Kurdish/Kurmanji (Kurmanc√Æ)",
        "ps": "Pashto (Ÿæ⁄öÿ™Ÿà)",
        "am": "Amharic (·ä†·àõ·à≠·äõ)",
        "ti": "Tigrinya (·âµ·åç·à≠·äõ)",

        # African Languages
        "sw": "Swahili (Kiswahili)",
        "ha": "Hausa (Hausa)",
        "yo": "Yoruba (Yor√πb√°)",
        "ig": "Igbo (Igbo)",
        "zu": "Zulu (isiZulu)",
        "xh": "Xhosa (isiXhosa)",
        "af": "Afrikaans (Afrikaans)",
        "st": "Sesotho (Sesotho)",
        "tn": "Setswana (Setswana)",
        "sn": "Shona (chiShona)",
        "rw": "Kinyarwanda (Ikinyarwanda)",
        "rn": "Kirundi (Ikirundi)",
        "lg": "Luganda (Luganda)",
        "ny": "Chichewa (Chiche≈µa)",
        "mg": "Malagasy (Malagasy)",
        "so": "Somali (Soomaali)",
        "om": "Oromo (Afaan Oromoo)",
        "wo": "Wolof (Wolof)",
        "ff": "Fulah (Fulfulde)",
        "ak": "Akan/Twi (Akan)",
        "ee": "Ewe (E ãegbe)",
        "ln": "Lingala (Ling√°la)",
        "kg": "Kongo (Kikongo)",

        # Pacific Languages
        "mi": "MƒÅori (Te Reo MƒÅori)",
        "haw": "Hawaiian ( ª≈ålelo Hawai ªi)",
        "sm": "Samoan (Gagana Samoa)",
        "to": "Tongan (Lea Fakatonga)",
        "fj": "Fijian (Vosa Vakaviti)",

        # Indigenous American Languages
        "qu": "Quechua (Runasimi)",
        "ay": "Aymara (Aymar aru)",
        "gn": "Guarani (Ava√±e'·∫Ω)",
        "nv": "Navajo (Din√© bizaad)",
        "chr": "Cherokee (·è£·é≥·é©)",
        "oj": "Ojibwe (Anishinaabemowin)",
        "cr": "Cree (·ìÄ·ê¶·êÉ·î≠·êç·êè·ê£)",
        "iu": "Inuktitut (·êÉ·ìÑ·íÉ·ëé·ëê·ë¶)",
        "kl": "Greenlandic (Kalaallisut)",

        # Caucasian Languages
        "ka": "Georgian (·É•·Éê·É†·Éó·É£·Éö·Éò)",
        "hy": "Armenian (’Ä’°’µ’•÷Ädelays)",
        "ab": "Abkhaz (–ê‘•—Å—É–∞)",
        "ce": "Chechen (–ù–æ—Ö—á–∏–π–Ω)",
        "av": "Avar (–ê–≤–∞—Ä)",
        "os": "Ossetian (–ò—Ä–æ–Ω)",

        # Sign Languages (written form descriptions)
        "ase": "American Sign Language (ASL)",
        "bfi": "British Sign Language (BSL)",
        "fsl": "French Sign Language (LSF)",

        # Constructed/Auxiliary Languages
        "eo": "Esperanto (Esperanto)",
        "ia": "Interlingua (Interlingua)",
        "vo": "Volap√ºk (Volap√ºk)",
        "io": "Ido (Ido)",
        "jbo": "Lojban (la .lojban.)",

        # Historical/Classical Languages
        "la": "Latin (Latina)",
        "grc": "Ancient Greek (·ºôŒªŒªŒ∑ŒΩŒπŒ∫ŒÆ)",
        "ang": "Old English (Englisc)",
        "non": "Old Norse (Norr≈ìnt)",
        "peo": "Old Persian (êé±êé†êéºêéø)",
        "egy": "Ancient Egyptian (r n km.t)",
        "akk": "Akkadian (íÄùíÖóíÅ∫íåë)",
        "sux": "Sumerian (íÖ¥íÇ†)",

        # Regional/Minority European Languages
        "oc": "Occitan (Occitan)",
        "br": "Breton (Brezhoneg)",
        "co": "Corsican (Corsu)",
        "sc": "Sardinian (Sardu)",
        "fur": "Friulian (Furlan)",
        "lad": "Ladino (Judeoespa√±ol)",
        "rm": "Romansh (Rumantsch)",
        "hsb": "Upper Sorbian (Hornjoserb≈°ƒáina)",
        "dsb": "Lower Sorbian (Dolnoserb≈°ƒáina)",
        "csb": "Kashubian (Kasz√´bsczi)",
        "szl": "Silesian (≈öl≈çnsko godka)",
        "frr": "North Frisian (Nordfriisk)",
        "fy": "West Frisian (Frysk)",
        "li": "Limburgish (Limburgs)",
        "wa": "Walloon (Walon)",
        "pms": "Piedmontese (Piemont√®is)",
        "nap": "Neapolitan (Napulitano)",
        "scn": "Sicilian (Sicilianu)",
        "vec": "Venetian (V√®neto)",
        "lmo": "Lombard (Lombard)",
        "eml": "Emilian-Romagnol (Emigli√†n-Rumagn√≤l)",

        # Other Languages
        "yi": "Yiddish (◊ô◊ô÷¥◊ì◊ô◊©)",
        "tl": "Tagalog (Tagalog)",
        "ht": "Haitian Creole (Krey√≤l Ayisyen)",
        "crs": "Seychellois Creole (Kreol Seselwa)",
        "pap": "Papiamento (Papiamentu)",
        "tet": "Tetum (Tetun)",
        "dv": "Dhivehi/Maldivian (ﬁãﬁ®ﬁàﬁ¨ﬁÄﬁ®)",
        "bo": "Tibetan (‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë)",
        "dz": "Dzongkha (‡Ω¢‡æ´‡Ωº‡ΩÑ‡ºã‡ΩÅ)",
        "ug": "Uyghur (ÿ¶€áŸäÿ∫€áÿ±⁄Ü€ï)",
        "tt": "Tatar (–¢–∞—Ç–∞—Ä—á–∞)",
        "ba": "Bashkir (–ë–∞—à“°–æ—Ä—Ç—Å–∞)",
        "cv": "Chuvash (–ß”ë–≤–∞—à–ª–∞)",
        "sah": "Yakut/Sakha (–°–∞—Ö–∞ —Ç—ã–ª–∞)",
        "tyv": "Tuvan (–¢—ã–≤–∞ –¥—ã–ª)",
        "bua": "Buryat (–ë—É—Ä—è–∞–¥)",
        "xal": "Kalmyk (–•–∞–ª—å–º–≥ –∫–µ–ª–Ω)",
    }

    def _get_language_name(self, lang_code: str) -> str:
        """
        Get full language name from ISO code.

        Supports 200+ world languages including:
        - All major world languages
        - Regional and minority languages
        - Indigenous languages
        - Historical/classical languages
        - Sign languages (written descriptions)
        """
        # Normalize the code (lowercase, handle variants)
        normalized = lang_code.lower().strip()

        # Direct lookup
        if normalized in self.LANGUAGE_MAP:
            return self.LANGUAGE_MAP[normalized]

        # Try without region suffix (e.g., "en-US" -> "en")
        base_code = normalized.split('-')[0].split('_')[0]
        if base_code in self.LANGUAGE_MAP:
            return self.LANGUAGE_MAP[base_code]

        # Return the code itself if unknown (AI can often still handle it)
        return f"Language: {lang_code}"

    def _is_small_ollama_model(self, provider_config: Optional[LLMProvider]) -> bool:
        """
        Check if the provider is an Ollama model with less than 1B parameters.

        This is used to simplify prompts for very small models that can't handle
        complex instructions with context segments.

        IMPORTANT: This check ONLY affects Ollama models. Cloud providers
        (OpenAI, Anthropic, Google, etc.) are NEVER affected.

        Args:
            provider_config: The LLM provider configuration

        Returns:
            True if this is a small (<1B) Ollama model, False otherwise
        """
        if provider_config is None:
            return False

        # Only check Ollama models - cloud providers are never "small"
        if provider_config.type != "ollama":
            return False

        model_name = provider_config.model.lower()

        # Parse size from model name (e.g., "gemma3:270m", "phi3:mini", "llama3.2:1b")
        # Common patterns: :270m, :500m, :1b, :3b, :7b, :8b, :70b
        import re

        # Look for size indicators in model name
        size_match = re.search(r':(\d+\.?\d*)(m|b)', model_name)
        if size_match:
            size_num = float(size_match.group(1))
            size_unit = size_match.group(2)

            if size_unit == 'm':  # Millions
                return size_num < 1000  # Less than 1000M = less than 1B
            elif size_unit == 'b':  # Billions
                return size_num < 1  # Less than 1B

        # Known small model patterns (no explicit size in name)
        small_model_patterns = [
            'mini', 'micro', 'nano', 'tiny', 'small',
            'phi-1', 'phi1',  # phi-1 is 1.3B but often struggles
        ]

        for pattern in small_model_patterns:
            if pattern in model_name:
                return True

        # Default: assume model is capable enough
        return False

    def _build_simplified_script_prompt(
        self,
        segments: List[SegmentDescription],
        style: ScriptStyle,
        video_title: Optional[str] = None,
        custom_instructions: Optional[str] = None
    ) -> Tuple[str, str]:
        """
        Build a simplified prompt for small Ollama models (<1B params).

        This prompt:
        - Has no context segments (small models copy them)
        - Uses minimal instructions
        - Focuses on direct description-to-script conversion

        IMPORTANT: This method is ONLY used for small Ollama models.
        Cloud providers and larger models use _build_script_prompt().
        """
        language_name = self._get_language_name(style.language)

        # Simple system prompt
        system_prompt = f"""You are a voice-over script writer. Write natural, spoken narration in {language_name}.
Write complete sentences that sound good when read aloud. Be concise and engaging."""

        # Build simple segment list
        segment_list = []
        for i, seg in enumerate(segments):
            duration = max(0.5, seg.end_time - seg.start_time)
            target_words = self.estimate_word_count(duration, style.language)
            segment_list.append(
                f"Segment {i+1}: {seg.description or 'General narration'} (about {target_words} words)"
            )

        # Simple user prompt - no context, no examples, just the task
        user_prompt = f"""Write a voice-over script for this video segment:

{chr(10).join(segment_list)}

{f'Note: {custom_instructions}' if custom_instructions else ''}

Write the narration script now:"""

        return system_prompt, user_prompt

    def _build_script_prompt(
        self,
        segments: List[SegmentDescription],
        style: ScriptStyle,
        video_title: Optional[str] = None,
        video_context: Optional[str] = None,
        custom_instructions: Optional[str] = None,
        fit_to_duration: bool = True,
        context_segments: Optional[List[ExistingSegmentContext]] = None,
        provider_config: Optional[LLMProvider] = None
    ) -> Tuple[str, str]:
        """
        Build the system and user prompts for script generation.

        Args:
            segments: List of segments to generate scripts for
            style: Style configuration for the scripts
            video_title: Optional title of the video project
            video_context: Optional additional context about the video
            custom_instructions: Optional custom instructions from user
            fit_to_duration: Whether to fit scripts to segment duration
            context_segments: Optional list of existing segments for narrative continuity
            provider_config: Optional provider config for model-aware prompting

        Returns:
            Tuple of (system_prompt, user_prompt)
        """
        system_prompt = self._get_system_prompt(style)
        language_name = self._get_language_name(style.language)

        # Check if we need simplified prompts for small Ollama models
        # This ONLY affects Ollama models <1B params, never cloud providers
        use_simplified_prompt = self._is_small_ollama_model(provider_config)
        if use_simplified_prompt:
            logger.info(f"[LLM Service] Using simplified prompt for small model: {provider_config.model}")
            return self._build_simplified_script_prompt(segments, style, video_title, custom_instructions)

        # Calculate target word counts for each segment
        segment_specs = []
        total_duration = 0
        for i, seg in enumerate(segments):
            duration = max(0.5, seg.end_time - seg.start_time)  # Ensure positive duration
            total_duration += duration
            target_words = self.estimate_word_count(duration, style.language)

            segment_specs.append(
                f"Segment {i+1}:\n"
                f"  ‚Ä¢ Duration: {duration:.1f} seconds\n"
                f"  ‚Ä¢ Required word count: {max(5, target_words)} words\n"
                f"  ‚Ä¢ Description: {seg.description or 'No specific description provided - write appropriate general narration'}" +
                (f"\n  ‚Ä¢ Keywords to include: {', '.join(seg.keywords)}" if seg.keywords else "")
            )

        # Handle missing video title
        title_text = video_title if video_title and video_title.strip() else "Video Project"

        # Handle missing context - provide guidance
        if video_context and video_context.strip():
            context_section = f"VIDEO CONTEXT:\n{video_context}"
        else:
            context_section = "VIDEO CONTEXT:\nNo additional context provided. Write narration based only on segment descriptions."

        # Build narrative context from existing segments
        narrative_context = ""
        if context_segments and len(context_segments) > 0:
            before_segments = [s for s in context_segments if s.position == "before"]
            after_segments = [s for s in context_segments if s.position == "after"]

            # Sort by time
            before_segments.sort(key=lambda s: s.start_time)
            after_segments.sort(key=lambda s: s.start_time)

            context_parts = []

            if before_segments:
                context_parts.append("PRECEDING SEGMENTS (for narrative continuity):")
                for seg in before_segments[-3:]:  # Limit to last 3 before segments
                    context_parts.append(
                        f"  [{seg.start_time:.1f}s - {seg.end_time:.1f}s] \"{seg.name}\":\n"
                        f"    Script: \"{seg.script[:300]}{'...' if len(seg.script) > 300 else ''}\""
                    )

            if after_segments:
                context_parts.append("\nFOLLOWING SEGMENTS (for narrative continuity):")
                for seg in after_segments[:3]:  # Limit to first 3 after segments
                    context_parts.append(
                        f"  [{seg.start_time:.1f}s - {seg.end_time:.1f}s] \"{seg.name}\":\n"
                        f"    Script: \"{seg.script[:300]}{'...' if len(seg.script) > 300 else ''}\""
                    )

            if context_parts:
                narrative_context = "\n\nNARRATIVE CONTEXT (for reference only):\n" + "\n".join(context_parts) + """

‚ö†Ô∏è CRITICAL - DO NOT COPY THE ABOVE SCRIPTS!
The existing scripts above are shown ONLY for context/reference.
You MUST write COMPLETELY NEW and ORIGINAL content based on the segment descriptions below.
- DO NOT copy or paraphrase the existing scripts
- DO NOT repeat phrases from the context
- CREATE fresh, original narration based ONLY on the new segment's description
- Maintain similar TONE but with DIFFERENT content"""

        # Build user prompt
        user_prompt = f"""Write voice-over narration scripts for the following video segments.

PROJECT: {title_text}

{context_section}{narrative_context}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
NEW SEGMENTS TO SCRIPT (write ORIGINAL content for these):
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{chr(10).join(segment_specs)}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

{f'ADDITIONAL INSTRUCTIONS FROM USER:{chr(10)}{custom_instructions}' if custom_instructions else ''}

CRITICAL REQUIREMENTS:
- Write in {language_name}
- Write EXACTLY {len(segments)} script(s) - one for EACH segment listed above
- Match the required word count for each segment
- Write complete, speakable narration - NO placeholders like "your text here"
- Base content ONLY on the segment descriptions provided
{('- Maintain similar tone as existing segments but with DIFFERENT content' if context_segments else '')}

Your response must contain real narration scripts based on each segment's description."""

        return system_prompt, user_prompt

    async def _generate_with_system_prompt(
        self,
        provider: BaseLLMProvider,
        system_prompt: str,
        user_prompt: str,
        max_tokens: int = 2000,
        temperature: float = 0.7
    ) -> str:
        """
        Generate text using system + user prompt pattern.

        Handles different provider types appropriately:
        - Ollama: Uses system parameter in generate()
        - LangChain providers: Uses chat with system message
        - Legacy providers: Combines prompts
        """
        # Check if provider supports system prompts natively
        if isinstance(provider, OllamaProvider):
            # Ollama supports system prompt directly
            return await provider.generate(
                user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt
            )
        elif hasattr(provider, 'chat'):
            # Use chat API with system message for providers that support it
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            return await provider.chat(messages, max_tokens=max_tokens, temperature=temperature)
        else:
            # Fallback: combine system and user prompt for providers without system support
            combined_prompt = f"""SYSTEM INSTRUCTIONS:
{system_prompt}

USER REQUEST:
{user_prompt}"""
            return await provider.generate(combined_prompt, max_tokens=max_tokens, temperature=temperature)

    async def generate_scripts(
        self,
        provider_config: LLMProvider,
        segments: List[SegmentDescription],
        style: ScriptStyle,
        video_title: Optional[str] = None,
        video_context: Optional[str] = None,
        custom_instructions: Optional[str] = None,
        fit_to_duration: bool = True,
        max_retries: int = 2,
        context_segments: Optional[List[ExistingSegmentContext]] = None
    ) -> Tuple[List[GeneratedScript], List[str]]:
        """
        Generate scripts for video segments using AI.

        Works with any LLM provider (OpenAI, Anthropic, Google, Ollama, etc.)
        with optimized prompts for accurate script generation.

        Args:
            provider_config: LLM provider configuration
            segments: List of segments to generate scripts for
            style: Script style configuration
            video_title: Optional video project title
            video_context: Optional additional context
            custom_instructions: Optional user instructions
            fit_to_duration: Whether to fit scripts to duration
            max_retries: Number of retries for failed generations
            context_segments: Optional existing segments for narrative continuity

        Returns:
            Tuple of (generated scripts, warnings)
        """
        provider = self.get_provider(provider_config)
        system_prompt, user_prompt = self._build_script_prompt(
            segments, style, video_title, video_context,
            custom_instructions, fit_to_duration, context_segments,
            provider_config=provider_config  # For model-aware prompting
        )

        warnings = []
        scripts = []
        current_custom_instructions = custom_instructions

        for attempt in range(max_retries + 1):
            try:
                # Try structured output first for reliable JSON generation
                # This works with: Ollama (native format), OpenAI/Anthropic (with_structured_output)
                # Reference: https://docs.langchain.com/docs/oss/langchain/structured-output
                parsed_scripts = None
                use_structured_output = hasattr(provider, 'generate_structured')

                if use_structured_output:
                    try:
                        # Combine system and user prompt for structured generation
                        full_prompt = f"{system_prompt}\n\n{user_prompt}"
                        structured_response = await provider.generate_structured(
                            prompt=full_prompt,
                            response_schema=LLMScriptGenerationOutput,
                            max_tokens=2000,
                            temperature=0.7
                        )
                        # Extract scripts from structured response
                        parsed_scripts = [item.script for item in structured_response.scripts]
                        logger.info(f"Structured output successful: {len(parsed_scripts)} scripts")
                    except Exception as e:
                        logger.warning(f"Structured output failed, falling back to text parsing: {e}")
                        use_structured_output = False

                # Fallback: Use text generation + manual parsing
                if not use_structured_output or parsed_scripts is None:
                    response = await self._generate_with_system_prompt(
                        provider, system_prompt, user_prompt,
                        max_tokens=2000, temperature=0.7
                    )
                    # Parse JSON from response using legacy method
                    parsed_scripts = self._parse_script_response(response, len(segments))

                for i, (seg, script_text) in enumerate(zip(segments, parsed_scripts)):
                    duration = max(0.5, seg.end_time - seg.start_time)  # Ensure positive duration
                    estimated_duration = self.estimate_duration(script_text, style.language)
                    word_count = len(script_text.split())
                    fits = estimated_duration <= duration * 1.1  # 10% tolerance

                    scripts.append(GeneratedScript(
                        segment_index=i,
                        start_time=seg.start_time,
                        end_time=seg.end_time,
                        text=script_text,
                        estimated_duration=estimated_duration,
                        word_count=word_count,
                        fits_duration=fits,
                        confidence=0.85 if fits else 0.6
                    ))

                    if not fits and fit_to_duration:
                        overflow = estimated_duration - duration
                        warnings.append(
                            f"Segment {i+1}: Script is ~{overflow:.1f}s longer than segment. "
                            f"Consider shortening or extending the segment."
                        )

                # If all scripts fit or we're not fitting to duration, we're done
                if not fit_to_duration or all(s.fits_duration for s in scripts):
                    break

                # If some scripts don't fit and we have retries left, try again with adjusted prompt
                if attempt < max_retries:
                    # Add more specific length constraints
                    current_custom_instructions = (current_custom_instructions or "") + \
                        " CRITICAL: Previous attempt had scripts that were too long. Write SHORTER, more concise narration."
                    system_prompt, user_prompt = self._build_script_prompt(
                        segments, style, video_title, video_context,
                        current_custom_instructions, fit_to_duration, context_segments,
                        provider_config=provider_config  # For model-aware prompting
                    )
                    scripts = []
                    logger.info(f"Retrying script generation (attempt {attempt + 2}/{max_retries + 1})")

            except Exception as e:
                logger.error(f"Script generation error: {e}")
                if attempt == max_retries:
                    raise
                await asyncio.sleep(1)

        return scripts, warnings

    def _clean_response(self, response: str) -> str:
        """Clean LLM response - remove thinking tags and other artifacts"""
        # Remove <think>...</think> tags (used by qwen, deepseek, etc.)
        cleaned = re.sub(r'<think>[\s\S]*?</think>', '', response, flags=re.IGNORECASE)
        # Remove other common artifacts
        cleaned = re.sub(r'<\|.*?\|>', '', cleaned)  # Remove special tokens
        cleaned = cleaned.strip()
        return cleaned

    def _is_valid_script(self, script: str) -> bool:
        """Check if a script is valid (not placeholder content)"""
        if not script or len(script.strip()) < 10:
            return False

        script_lower = script.lower().strip()

        # Check for placeholder patterns
        placeholder_patterns = [
            'your narration', 'narration text here', 'script here',
            'your script here', '[your', 'placeholder', 'text here',
            'your text', 'write your', 'insert your', 'add your',
            'example text', 'sample text', 'lorem ipsum'
        ]
        if any(phrase in script_lower for phrase in placeholder_patterns):
            return False

        # Check if script is just punctuation or very short
        if script.strip() in ['...', '..', '.', '-', '--', '‚Äî', '']:
            return False

        # Check if mostly non-alphanumeric
        alpha_count = sum(1 for c in script if c.isalpha())
        if alpha_count < 10:
            return False

        return True

    def _extract_script_from_nested_json(self, text: str) -> Optional[str]:
        """
        Extract script text from nested JSON if the text itself is a JSON array.

        Some smaller LLMs return JSON within the script/text field instead of plain text.
        This handles cases like:
        - text: '[{"segment": 1, "script": "actual script"}]'
        - text: '{"script": "actual script"}'

        Returns:
            Extracted script text if found, None otherwise
        """
        if not text or not text.strip():
            return None

        text = text.strip()

        # Check if it looks like JSON
        if not (text.startswith('[') or text.startswith('{')):
            return None

        try:
            data = json.loads(text)

            # Case 1: Array of segment objects
            if isinstance(data, list) and len(data) > 0:
                first_item = data[0]
                if isinstance(first_item, dict):
                    script = first_item.get("script", first_item.get("text", ""))
                    if script and isinstance(script, str) and self._is_valid_script(script):
                        logger.info(f"Extracted script from nested JSON array: {script[:50]}...")
                        return script

            # Case 2: Single object with script field
            elif isinstance(data, dict):
                script = data.get("script", data.get("text", ""))
                if script and isinstance(script, str) and self._is_valid_script(script):
                    logger.info(f"Extracted script from nested JSON object: {script[:50]}...")
                    return script

        except json.JSONDecodeError:
            # Not valid JSON, return None
            pass

        return None

    def _parse_script_response(self, response: str, expected_count: int) -> List[str]:
        """Parse LLM response to extract scripts"""
        # Check for empty response first
        if not response or not response.strip():
            logger.error("LLM returned empty response. This usually means:")
            logger.error("  1. The model is too small for this task (try a larger model like llama3.2:3b or qwen3:4b)")
            logger.error("  2. The model name is invalid or not installed in Ollama")
            logger.error("  3. The prompt was too long for the model's context window")
            raise ValueError("LLM returned empty response. Try using a larger/different model.")

        # First clean the response
        response = self._clean_response(response)

        # Check again after cleaning
        if not response or not response.strip():
            logger.error("LLM response was empty after cleaning thinking tags")
            raise ValueError("LLM response was empty after processing. The model may not support this task.")

        # Try to extract JSON from response
        try:
            # Find all JSON arrays and pick the best one
            candidates = []
            for match in re.finditer(r'\[[\s\S]*?\]', response):
                try:
                    candidate_text = match.group()
                    data = json.loads(candidate_text)
                    if isinstance(data, list) and len(data) > 0:
                        scripts = []
                        for item in data:
                            if isinstance(item, dict):
                                raw_script = item.get("script", item.get("text", ""))

                                # Check if the script/text is itself a nested JSON
                                # This handles cases where smaller LLMs return JSON within the text field
                                if raw_script and isinstance(raw_script, str):
                                    nested_script = self._extract_script_from_nested_json(raw_script)
                                    if nested_script:
                                        scripts.append(nested_script)
                                    else:
                                        scripts.append(raw_script)
                                else:
                                    scripts.append(raw_script if raw_script else "")
                            elif isinstance(item, str):
                                # Check if the string itself is nested JSON
                                nested_script = self._extract_script_from_nested_json(item)
                                scripts.append(nested_script if nested_script else item)

                        # Validate ALL scripts are valid (not placeholders)
                        all_valid = all(self._is_valid_script(s) for s in scripts)
                        if scripts and all_valid:
                            total_len = sum(len(s) for s in scripts)
                            candidates.append((scripts, total_len))
                        elif scripts:
                            invalid_scripts = [s[:50] for s in scripts if not self._is_valid_script(s)]
                            logger.warning(f"Skipping invalid/placeholder scripts: {invalid_scripts}")
                except json.JSONDecodeError:
                    continue

            # Return the candidate with the most content
            if candidates:
                best = max(candidates, key=lambda x: x[1])
                logger.info(f"Parsed {len(best[0])} scripts with {best[1]} total chars")
                return best[0]

        except Exception as e:
            logger.debug(f"JSON extraction failed: {e}")

        # Fallback: Try to extract scripts from markdown-style or plain text
        # Look for numbered patterns like "1." or "Script 1:" etc.
        script_patterns = [
            r'(?:Script\s*)?(\d+)[.:]\s*"([^"]+)"',  # Script 1: "text" or 1: "text"
            r'(?:Script\s*)?(\d+)[.:]\s*([^\n]+)',    # Script 1: text or 1: text
        ]

        for pattern in script_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            if matches and len(matches) >= expected_count:
                scripts = [m[1].strip().strip('"\'') for m in matches[:expected_count]]
                if all(self._is_valid_script(s) for s in scripts):
                    logger.info(f"Extracted {len(scripts)} scripts from text patterns")
                    return scripts

        # Fallback: split by segment markers
        parts = re.split(r'Segment \d+:', response, flags=re.IGNORECASE)
        parts = [p.strip() for p in parts if p.strip() and self._is_valid_script(p)]

        if len(parts) >= expected_count:
            return parts[:expected_count]

        # Last fallback: if no valid scripts found, raise an error
        # This will trigger a retry with adjusted prompt
        logger.error(f"Failed to parse valid scripts from response. Response preview: {response[:500]}")
        raise ValueError(f"LLM returned invalid/placeholder scripts. Response did not contain usable narration text.")

    async def refine_script(
        self,
        provider_config: LLMProvider,
        original_text: str,
        target_duration: float,
        current_duration: float,
        instruction: str,
        custom_feedback: Optional[str] = None,
        language: str = "en"
    ) -> Tuple[str, float]:
        """
        Refine a script based on feedback

        Returns:
            Tuple of (refined text, estimated duration)
        """
        provider = self.get_provider(provider_config)

        if instruction == "shorten":
            target_words = self.estimate_word_count(target_duration, language)
            action = f"Shorten this script to approximately {target_words} words while keeping the key message."
        elif instruction == "lengthen":
            target_words = self.estimate_word_count(target_duration, language)
            action = f"Expand this script to approximately {target_words} words with more detail."
        elif instruction == "rephrase":
            action = "Rephrase this script while keeping the same length and meaning."
        elif instruction == "simplify":
            action = "Simplify this script using simpler words while keeping the same length."
        elif instruction == "make_formal":
            action = "Make this script more formal and professional while keeping the same length."
        elif instruction == "make_casual":
            action = "Make this script more casual and conversational while keeping the same length."
        else:
            action = "Improve this script."

        prompt = f"""Refine the following video narration script.

**Current Script:**
{original_text}

**Current Duration:** ~{current_duration:.1f} seconds
**Target Duration:** ~{target_duration:.1f} seconds

**Task:** {action}

{f'**Additional Feedback:** {custom_feedback}' if custom_feedback else ''}

**Instructions:**
- Maintain the core message and intent
- Keep it suitable for voice-over narration
- Return ONLY the refined script text, no explanations

**Refined Script:**"""

        response = await provider.generate(prompt, max_tokens=500, temperature=0.6)
        refined_text = response.strip()
        estimated_duration = self.estimate_duration(refined_text, language)

        return refined_text, estimated_duration

    async def check_health(self) -> Dict:
        """Check availability of all LLM providers"""
        health = {
            "ollama_available": False,
            "ollama_models": [],
            "openai_configured": False,
            "anthropic_configured": False,
            "google_configured": False,
            "azure_openai_configured": False,
            "aws_bedrock_configured": False,
            "huggingface_configured": False,
            "langchain_available": LANGCHAIN_AVAILABLE,
        }

        # Check Ollama
        try:
            health["ollama_available"] = await self.ollama_provider.is_available()
            if health["ollama_available"]:
                models = await self.ollama_provider.list_models()
                health["ollama_models"] = [m.name for m in models]
        except Exception as e:
            logger.debug(f"[LLM Service] Ollama health check failed: {e}")

        # Check configured API keys from environment
        health["openai_configured"] = bool(os.getenv("OPENAI_API_KEY"))
        health["anthropic_configured"] = bool(os.getenv("ANTHROPIC_API_KEY"))
        health["google_configured"] = bool(os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"))
        health["azure_openai_configured"] = bool(
            os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        health["aws_bedrock_configured"] = bool(
            os.getenv("AWS_ACCESS_KEY_ID") and os.getenv("AWS_SECRET_ACCESS_KEY")
        )
        health["huggingface_configured"] = bool(os.getenv("HUGGINGFACEHUB_API_TOKEN"))

        logger.debug(f"[LLM Service] Health check: {health}")
        return health

    def _build_auto_segment_prompt(
        self,
        video_duration: float,
        video_description: str,
        style: ScriptStyle,
        video_title: Optional[str] = None,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0,
        custom_instructions: Optional[str] = None,
    ) -> Tuple[str, str]:
        """
        Build system and user prompts for automatic segment creation.

        Returns:
            Tuple of (system_prompt, user_prompt)
        """
        system_prompt = self._get_system_prompt(style)
        language_name = self._get_language_name(style.language)

        # Calculate recommended number of segments
        avg_segment = (min_segment_duration + max_segment_duration) / 2
        recommended_segments = max(1, int(video_duration / avg_segment))

        # Word count calculations for guidance
        wps = self.WORDS_PER_SECOND.get(style.language, 2.5)

        # Handle missing video title/description
        title_text = video_title if video_title and video_title.strip() else "Video Project"
        description_text = video_description if video_description and video_description.strip() else "No description provided"

        user_prompt = f"""Create a complete voice-over script divided into segments for a video.

PROJECT: {title_text}
DURATION: {video_duration:.0f} seconds
LANGUAGE: {language_name}

VIDEO DESCRIPTION:
{description_text}

SEGMENTATION REQUIREMENTS:
- Divide the video into {recommended_segments} logical segments
- Each segment should be {min_segment_duration:.0f}-{max_segment_duration:.0f} seconds
- Segments must cover from 0 to {video_duration:.0f} seconds
- Segments must NOT overlap (each end_time = next start_time)
- Last segment must end at exactly {video_duration:.1f}

WORD COUNT FORMULA (for {language_name}):
- Words needed = duration √ó {wps:.1f}
- Example: 10 second segment = {int(10 * wps)} words

IMPORTANT:
- Base ALL content strictly on the provided description
- Do NOT invent facts, statistics, or details not mentioned
- If description is vague, write appropriate generic narration
- Each script must be complete, speakable text
{f'- Additional requirements: {custom_instructions}' if custom_instructions else ''}

OUTPUT FORMAT (JSON array only, no other text):
[
  {{
    "name": "Opening",
    "start_time": 0.0,
    "end_time": 8.0,
    "description": "Brief description of segment purpose",
    "script": "Your complete {language_name} narration text here with exact word count for duration..."
  }},
  {{
    "name": "Next Section",
    "start_time": 8.0,
    "end_time": 18.0,
    "description": "What this segment covers",
    "script": "Continue with matching narration..."
  }}
]"""

        return system_prompt, user_prompt

    async def auto_generate_segments(
        self,
        provider_config: LLMProvider,
        video_duration: float,
        video_description: str,
        style: ScriptStyle,
        video_title: Optional[str] = None,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0,
        custom_instructions: Optional[str] = None,
        max_retries: int = 3
    ) -> Tuple[List[AutoGeneratedSegment], List[str]]:
        """
        Automatically generate segments with timing and scripts from video description.

        Works with any LLM provider (OpenAI, Anthropic, Google, Ollama, etc.)
        with optimized prompts for accurate segment generation.

        Returns:
            Tuple of (generated segments, warnings)
        """
        provider = self.get_provider(provider_config)

        warnings = []
        segments = []
        last_error = None
        current_custom_instructions = custom_instructions

        for attempt in range(max_retries + 1):
            try:
                # Build prompt with retry-specific instructions
                retry_instructions = current_custom_instructions or ""
                if attempt > 0:
                    retry_instructions += "\n\nCRITICAL: Previous attempt failed. Write ACTUAL narration scripts with real complete sentences. NO placeholders, NO '...', NO '[text here]'."

                system_prompt, user_prompt = self._build_auto_segment_prompt(
                    video_duration, video_description, style, video_title,
                    min_segment_duration, max_segment_duration, retry_instructions if retry_instructions else None
                )

                # Use lower temperature on retries for more deterministic output
                temperature = 0.7 if attempt == 0 else 0.5

                logger.info(f"Auto-segment generation attempt {attempt + 1}/{max_retries + 1}")

                # Try structured output first for reliable JSON generation
                parsed_segments = None
                use_structured_output = hasattr(provider, 'generate_structured')

                if use_structured_output:
                    try:
                        full_prompt = f"{system_prompt}\n\n{user_prompt}"
                        structured_response = await provider.generate_structured(
                            prompt=full_prompt,
                            response_schema=LLMAutoSegmentOutput,
                            max_tokens=3000,
                            temperature=temperature
                        )
                        # Convert structured response to dict format
                        parsed_segments = []
                        for item in structured_response.segments:
                            # Validate script content
                            if not self._is_valid_script(item.script):
                                logger.warning(f"Invalid script in structured output: '{item.script[:50]}...'")
                                raise ValueError("LLM returned placeholder scripts")

                            # Ensure timing is valid
                            end_time = min(item.end_time, video_duration)
                            if end_time <= item.start_time:
                                end_time = min(item.start_time + 10, video_duration)

                            parsed_segments.append({
                                "name": item.name,
                                "start_time": item.start_time,
                                "end_time": end_time,
                                "description": item.description,
                                "script": item.script
                            })
                        logger.info(f"Structured output successful: {len(parsed_segments)} segments")
                    except Exception as e:
                        logger.warning(f"Structured output failed, falling back to text parsing: {e}")
                        parsed_segments = None

                # Fallback: Use text generation + manual parsing
                if parsed_segments is None:
                    response = await self._generate_with_system_prompt(
                        provider, system_prompt, user_prompt,
                        max_tokens=3000, temperature=temperature
                    )
                    # Parse the response (will raise ValueError if scripts are invalid)
                    parsed_segments = self._parse_auto_segment_response(response, video_duration)

                for i, seg_data in enumerate(parsed_segments):
                    duration = seg_data["end_time"] - seg_data["start_time"]
                    script_text = seg_data.get("script", "")
                    estimated_duration = self.estimate_duration(script_text, style.language)
                    word_count = len(script_text.split())
                    fits = estimated_duration <= duration * 1.1  # 10% tolerance

                    segments.append(AutoGeneratedSegment(
                        segment_index=i,
                        name=seg_data.get("name", f"Segment {i+1}"),
                        start_time=seg_data["start_time"],
                        end_time=seg_data["end_time"],
                        description=seg_data.get("description", ""),
                        script=script_text,
                        estimated_duration=estimated_duration,
                        word_count=word_count,
                        fits_duration=fits
                    ))

                    if not fits:
                        overflow = estimated_duration - duration
                        warnings.append(
                            f"Segment '{seg_data.get('name', i+1)}': Script is ~{overflow:.1f}s longer than segment duration."
                        )

                # Validate segments don't overlap and are within video duration
                for i in range(len(segments) - 1):
                    if segments[i].end_time > segments[i+1].start_time:
                        warnings.append(
                            f"Segments '{segments[i].name}' and '{segments[i+1].name}' overlap."
                        )

                if segments and segments[-1].end_time > video_duration:
                    warnings.append(
                        f"Last segment extends beyond video duration ({segments[-1].end_time:.1f}s > {video_duration:.1f}s)"
                    )

                logger.info(f"Successfully generated {len(segments)} segments")
                break

            except ValueError as e:
                # Invalid script content - retry with modified prompt
                last_error = e
                logger.warning(f"Invalid scripts on attempt {attempt + 1}: {e}")
                segments = []  # Clear any partial segments

                if attempt < max_retries:
                    logger.info(f"Retrying with more explicit instructions...")
                    await asyncio.sleep(0.5)
                else:
                    raise RuntimeError(f"Failed to generate valid scripts after {max_retries + 1} attempts: {e}")

            except Exception as e:
                last_error = e
                logger.error(f"Auto-segment generation error (attempt {attempt + 1}): {e}")
                segments = []

                if attempt < max_retries:
                    await asyncio.sleep(1)
                else:
                    raise RuntimeError(f"Script generation failed: {e}")

        return segments, warnings

    def _parse_auto_segment_response(self, response: str, video_duration: float) -> List[Dict]:
        """Parse LLM response for auto-generated segments"""
        # Clean the response first
        response = self._clean_response(response)

        try:
            # Find JSON array in response
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                data = json.loads(json_match.group())
                segments = []
                has_valid_scripts = True

                for item in data:
                    if isinstance(item, dict):
                        # Validate required fields
                        start_time = float(item.get("start_time", 0))
                        end_time = float(item.get("end_time", start_time + 10))

                        # Ensure end_time doesn't exceed video duration
                        end_time = min(end_time, video_duration)

                        # Ensure valid timing
                        if end_time <= start_time:
                            end_time = min(start_time + 10, video_duration)

                        script = item.get("script", item.get("text", ""))

                        # Check if script is valid
                        if not self._is_valid_script(script):
                            logger.warning(f"Invalid script detected: '{script[:50]}...'")
                            has_valid_scripts = False

                        segments.append({
                            "name": item.get("name", f"Segment {len(segments) + 1}"),
                            "start_time": start_time,
                            "end_time": end_time,
                            "description": item.get("description", ""),
                            "script": script
                        })

                # If we have segments but invalid scripts, raise to trigger retry
                if segments and not has_valid_scripts:
                    raise ValueError("LLM returned placeholder scripts instead of real narration")

                if segments:
                    logger.info(f"Successfully parsed {len(segments)} segments with valid scripts")
                    return segments

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            logger.debug(f"Response preview: {response[:500]}")
        except ValueError as e:
            # Re-raise ValueError to trigger retry
            raise

        # If we get here, parsing failed completely
        logger.error(f"Failed to parse segments from response. Preview: {response[:300]}")
        raise ValueError(f"Could not parse valid segments from LLM response")


# Singleton instance
llm_service = LLMService()
