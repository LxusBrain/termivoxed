"""
LLM Service - Unified interface for multiple LLM providers

Supports:
- Ollama (local, free) with structured outputs
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Google (Gemini)
- Custom endpoints

Structured Outputs:
Ollama supports structured outputs via the `format` parameter, which accepts
a JSON schema. This ensures the model returns valid JSON matching the schema.
Reference: https://ollama.com/blog/structured-outputs

Video Understanding:
Supports video analysis using vision-language models (VLMs) like qwen3-vl.
Videos are processed by extracting key frames and sending them as images.
The Chat API with images in message provides best results.
"""

import asyncio
import base64
import json
import re
import subprocess
import tempfile
import os
from typing import Optional, List, Dict, Tuple, Any, Type
from abc import ABC, abstractmethod
from pydantic import BaseModel
from pathlib import Path

import httpx

from web_ui.api.schemas.llm_schemas import (
    LLMProvider,
    SegmentDescription,
    ScriptStyle,
    GeneratedScript,
    OllamaModel,
    AutoGeneratedSegment,
)
from utils.logger import logger


# ============================================================================
# Pydantic models for structured outputs
# ============================================================================

class VideoSegment(BaseModel):
    """Single video segment with script"""
    name: str
    start_time: float
    end_time: float
    description: str
    script: str


class VideoSegmentList(BaseModel):
    """List of video segments - used for structured output"""
    segments: List[VideoSegment]


class BaseLLMProvider(ABC):
    """Base class for LLM providers"""

    def __init__(self, api_key: Optional[str] = None, endpoint: Optional[str] = None):
        self.api_key = api_key
        self.endpoint = endpoint

    @abstractmethod
    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text from prompt"""
        pass

    @abstractmethod
    async def is_available(self) -> bool:
        """Check if provider is available"""
        pass


class OllamaProvider(BaseLLMProvider):
    """
    Ollama local LLM provider

    Supports the Ollama API including:
    - /api/generate for text generation
    - /api/chat for chat-based generation
    - think parameter for controlling thinking mode in reasoning models

    Reference: https://github.com/ollama/ollama/blob/main/docs/api.md
    """

    DEFAULT_ENDPOINT = "http://localhost:11434"

    # Models known to support thinking mode
    THINKING_MODELS = ['qwen3', 'deepseek-r1', 'qwq']

    def __init__(self, model: str, endpoint: Optional[str] = None, **kwargs):
        super().__init__(endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model
        # Check if this model supports thinking mode
        self._supports_thinking = any(m in self.model.lower() for m in self.THINKING_MODELS)

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        think: Optional[bool] = None,
        system: Optional[str] = None
    ) -> str:
        """
        Generate text using Ollama API

        Args:
            prompt: The input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0 - 1.0)
            think: Enable/disable thinking mode for reasoning models.
                   None = auto (disable for JSON generation tasks)
                   True = enable thinking (returns thinking + response separately)
                   False = disable thinking (direct response only)
            system: Optional system prompt

        Returns:
            Generated text (content only, thinking is separated if enabled)
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        # Build request payload
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Add system prompt if provided
        if system:
            payload["system"] = system

        # Handle thinking mode for supported models
        # Per Ollama docs: think=false disables thinking, response comes directly
        if self._supports_thinking:
            if think is None:
                # Default: disable thinking for cleaner JSON output
                payload["think"] = False
                logger.info(f"Auto-disabled thinking for model: {self.model}")
            else:
                payload["think"] = think
                logger.info(f"Thinking mode {'enabled' if think else 'disabled'} for: {self.model}")

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                logger.info(f"Calling Ollama model: {self.model} with {len(prompt)} char prompt")
                response = await client.post(
                    f"{self.endpoint}/api/generate",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                result = data.get("response", "")

                # Log if thinking was present (for debugging)
                if "thinking" in data:
                    logger.debug(f"Model returned thinking content ({len(data.get('thinking', ''))} chars)")

                logger.info(f"Ollama generation successful: {len(result)} chars")
                return result

            except httpx.TimeoutException as e:
                error_msg = f"Ollama request timed out after 300s: {str(e) or 'No additional details'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except httpx.HTTPStatusError as e:
                error_msg = f"Ollama HTTP error {e.response.status_code}: {e.response.text[:200] if e.response.text else 'No response body'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except Exception as e:
                error_msg = f"Ollama generation failed: {type(e).__name__}: {str(e) or 'Unknown error'}"
                logger.error(f"Ollama generation error: {error_msg}")
                raise RuntimeError(error_msg) from e

    async def chat(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
        think: Optional[bool] = None
    ) -> str:
        """
        Chat-based generation using Ollama /api/chat endpoint

        Args:
            messages: List of message dicts with 'role' and 'content' keys
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            think: Enable/disable thinking mode

        Returns:
            Assistant's response content
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Handle thinking mode
        if self._supports_thinking:
            payload["think"] = think if think is not None else False

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                message = data.get("message", {})
                return message.get("content", "")

            except Exception as e:
                logger.error(f"Ollama chat error: {e}")
                raise

    async def generate_structured(
        self,
        prompt: str,
        response_schema: Type[BaseModel],
        max_tokens: int = 4000,
        temperature: float = 0.7,
        system: Optional[str] = None
    ) -> BaseModel:
        """
        Generate structured output using Ollama's format parameter.

        This forces the model to return valid JSON matching the provided schema,
        eliminating parsing issues with thinking models like qwen3.

        Reference: https://ollama.com/blog/structured-outputs

        Args:
            prompt: The input prompt
            response_schema: A Pydantic model class defining the expected response structure
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (lower = more deterministic)
            system: Optional system prompt

        Returns:
            Validated Pydantic model instance
        """
        timeout = httpx.Timeout(300.0, connect=10.0)

        # Get JSON schema from Pydantic model
        json_schema = response_schema.model_json_schema()

        # Build messages for chat endpoint
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "format": json_schema,  # This is the key: structured output format
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }

        # Note: When using format parameter, thinking mode is automatically handled
        # The model will output structured JSON directly

        async with httpx.AsyncClient(timeout=timeout) as client:
            try:
                logger.info(f"Calling Ollama with structured output for model: {self.model}")
                logger.debug(f"Schema: {json_schema}")

                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                content = data.get("message", {}).get("content", "")

                logger.info(f"Ollama structured generation successful: {len(content)} chars")
                logger.debug(f"Raw content: {content[:500]}...")

                # Parse and validate the response using Pydantic
                result = response_schema.model_validate_json(content)
                return result

            except httpx.TimeoutException as e:
                error_msg = f"Ollama request timed out: {str(e) or 'No details'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except httpx.HTTPStatusError as e:
                error_msg = f"Ollama HTTP error {e.response.status_code}: {e.response.text[:200] if e.response.text else 'No body'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except json.JSONDecodeError as e:
                error_msg = f"Failed to parse Ollama response as JSON: {e}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e
            except Exception as e:
                error_msg = f"Ollama structured generation failed: {type(e).__name__}: {str(e) or 'Unknown error'}"
                logger.error(f"Ollama structured generation error: {error_msg}")
                raise RuntimeError(error_msg) from e

    async def is_available(self) -> bool:
        """Check if Ollama is running"""
        async with httpx.AsyncClient(timeout=5.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/api/tags")
                return response.status_code == 200
            except:
                return False

    async def list_models(self) -> List[OllamaModel]:
        """List available Ollama models"""
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/api/tags")
                response.raise_for_status()
                data = response.json()
                models = []
                for model in data.get("models", []):
                    models.append(OllamaModel(
                        name=model.get("name", "unknown"),
                        size=self._format_size(model.get("size", 0)),
                        modified_at=model.get("modified_at", ""),
                        digest=model.get("digest", "")[:12]
                    ))
                return models
            except Exception as e:
                logger.error(f"Failed to list Ollama models: {e}")
                return []

    def _format_size(self, size_bytes: int) -> str:
        """Format bytes to human readable"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"

    # =========================================================================
    # Video Understanding Methods (for VLMs like qwen3-vl)
    # =========================================================================

    async def extract_video_frames(
        self,
        video_path: str,
        num_frames: int = 4,
        max_dimension: int = 1280
    ) -> Tuple[List[str], float]:
        """
        Extract key frames from a video for analysis.

        Args:
            video_path: Path to the video file
            num_frames: Number of frames to extract (evenly distributed)
            max_dimension: Maximum width/height for frames (for efficiency)

        Returns:
            Tuple of (list of base64-encoded frames, video duration in seconds)
        """
        # Get video duration using ffprobe
        probe_cmd = [
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_format', '-show_streams', video_path
        ]
        result = subprocess.run(probe_cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise RuntimeError(f"ffprobe failed: {result.stderr}")

        probe_data = json.loads(result.stdout)
        duration = float(probe_data['format']['duration'])

        # Calculate frame extraction timestamps
        interval = duration / (num_frames + 1)
        frames_base64 = []

        with tempfile.TemporaryDirectory() as tmpdir:
            for i in range(num_frames):
                timestamp = interval * (i + 1)
                frame_path = os.path.join(tmpdir, f"frame_{i}.jpg")

                # Extract frame using ffmpeg with scaling
                cmd = [
                    'ffmpeg', '-y', '-ss', str(timestamp), '-i', video_path,
                    '-vframes', '1',
                    '-vf', f'scale=min({max_dimension}\\,iw):min({max_dimension}\\,ih):force_original_aspect_ratio=decrease',
                    '-q:v', '2', frame_path
                ]
                subprocess.run(cmd, capture_output=True)

                if os.path.exists(frame_path):
                    with open(frame_path, 'rb') as f:
                        frame_base64 = base64.b64encode(f.read()).decode('utf-8')
                        frames_base64.append(frame_base64)
                        logger.debug(f"Extracted frame {i+1} at {timestamp:.2f}s")

        logger.info(f"Extracted {len(frames_base64)} frames from {duration:.2f}s video")
        return frames_base64, duration

    async def analyze_video(
        self,
        video_path: str,
        prompt: Optional[str] = None,
        num_frames: int = 4,
        vision_model: str = "qwen3-vl:8b"
    ) -> Dict[str, Any]:
        """
        Analyze a video using a vision-language model.

        Extracts key frames from the video and sends them to a VLM for analysis.
        Best used with models like qwen3-vl that support image understanding.

        Args:
            video_path: Path to the video file
            prompt: Custom prompt for analysis. If None, uses default description prompt.
            num_frames: Number of frames to extract (default: 4)
            vision_model: The vision model to use (default: qwen3-vl:8b)

        Returns:
            Dict with keys:
                - description: The video description/analysis
                - duration: Video duration in seconds
                - frames_analyzed: Number of frames sent to model
                - success: Whether analysis succeeded
                - error: Error message if failed
        """
        result = {
            "description": "",
            "duration": 0.0,
            "frames_analyzed": 0,
            "success": False,
            "error": None
        }

        try:
            # Extract frames
            frames_base64, duration = await self.extract_video_frames(
                video_path, num_frames=num_frames
            )
            result["duration"] = duration
            result["frames_analyzed"] = len(frames_base64)

            if not frames_base64:
                result["error"] = "Failed to extract frames from video"
                return result

            # Default prompt for video analysis
            if prompt is None:
                prompt = f"""Analyze these {len(frames_base64)} frames extracted from a video and provide:
1. A detailed description of what the video shows
2. The main subject or topic
3. Key visual elements, text, or objects visible
4. The overall mood or style of the video
5. Any actions or movements occurring

Provide a comprehensive description that would help understand this video without watching it."""

            # Build chat request with images
            payload = {
                "model": vision_model,
                "messages": [{
                    "role": "user",
                    "content": prompt,
                    "images": frames_base64
                }],
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 2000,
                }
            }

            timeout = httpx.Timeout(180.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                logger.info(f"Sending {len(frames_base64)} frames to {vision_model} for analysis")

                response = await client.post(
                    f"{self.endpoint}/api/chat",
                    json=payload
                )
                response.raise_for_status()

                data = response.json()
                message = data.get("message", {})

                # Get content, fallback to thinking if content is empty
                content = message.get("content", "")
                thinking = message.get("thinking", "")

                # Use content if available, otherwise use thinking
                description = content if content else thinking

                # Clean up the description
                description = self._clean_video_description(description)

                if description:
                    result["description"] = description
                    result["success"] = True
                    logger.info(f"Video analysis successful: {len(description)} chars")
                else:
                    result["error"] = "Model returned empty response"
                    logger.warning("Video analysis returned empty response")

        except subprocess.SubprocessError as e:
            result["error"] = f"Video processing error: {str(e)}"
            logger.error(f"Video frame extraction failed: {e}")
        except httpx.TimeoutException:
            result["error"] = "Analysis timed out"
            logger.error("Video analysis request timed out")
        except httpx.HTTPStatusError as e:
            result["error"] = f"API error: {e.response.status_code}"
            logger.error(f"Video analysis HTTP error: {e}")
        except Exception as e:
            result["error"] = f"Analysis failed: {str(e)}"
            logger.error(f"Video analysis error: {type(e).__name__}: {e}")

        return result

    def _clean_video_description(self, description: str) -> str:
        """Clean up video description from model output"""
        if not description:
            return ""

        # Remove thinking tags if present
        description = re.sub(r'<think>[\s\S]*?</think>', '', description, flags=re.IGNORECASE)
        description = re.sub(r'<\|.*?\|>', '', description)  # Remove special tokens

        # Remove meta-commentary and thinking patterns
        # These patterns indicate the model is thinking about the task rather than describing
        meta_patterns = [
            r"(Got it|Okay|OK|Alright),?\s*(let's|let me|I'll|I will)\s+[^.]*\.\s*",
            r"(First|Starting|Beginning),?\s+I\s+(need to|will|should)\s+[^.]*\.\s*",
            r"Wait,?\s+[^.]*\.\s*",
            r"Looking at\s+(the\s+)?(user's|this|these)\s+[^.]*,?\s*",
            r"The user('s)?\s+(provided|says|input|message)[^.]*\.\s*",
            r"\[img\]",  # Image placeholder tokens
        ]
        for pattern in meta_patterns:
            description = re.sub(pattern, '', description, flags=re.IGNORECASE)

        # Clean up common prefixes from thinking mode
        thinking_prefixes = [
            r"^(So,?\s+)?let('s|me)\s+(look|analyze|describe|examine|break)",
            r"^(Okay|OK),?\s+(so\s+)?(the\s+)?(user|image|video)",
            r"^(Hmm|Well|Now),?\s+",
        ]
        for prefix in thinking_prefixes:
            description = re.sub(prefix, '', description, flags=re.IGNORECASE)

        # Remove multiple consecutive newlines
        description = re.sub(r'\n{3,}', '\n\n', description)

        return description.strip()


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API provider"""

    DEFAULT_ENDPOINT = "https://api.openai.com/v1"

    def __init__(self, api_key: str, model: str = "gpt-4-turbo-preview", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using OpenAI API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"OpenAI generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if OpenAI API is accessible"""
        if not self.api_key:
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(
                    f"{self.endpoint}/models",
                    headers={"Authorization": f"Bearer {self.api_key}"}
                )
                return response.status_code == 200
            except:
                return False


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude API provider"""

    DEFAULT_ENDPOINT = "https://api.anthropic.com/v1"

    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using Anthropic API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/messages",
                    headers={
                        "x-api-key": self.api_key,
                        "Content-Type": "application/json",
                        "anthropic-version": "2023-06-01"
                    },
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["content"][0]["text"]
            except Exception as e:
                logger.error(f"Anthropic generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if Anthropic API is accessible"""
        return bool(self.api_key)


class GoogleProvider(BaseLLMProvider):
    """Google Gemini API provider"""

    DEFAULT_ENDPOINT = "https://generativelanguage.googleapis.com/v1beta"

    def __init__(self, api_key: str, model: str = "gemini-pro", endpoint: Optional[str] = None, **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint or self.DEFAULT_ENDPOINT)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using Google Gemini API"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.endpoint}/models/{self.model}:generateContent",
                    params={"key": self.api_key},
                    json={
                        "contents": [{"parts": [{"text": prompt}]}],
                        "generationConfig": {
                            "maxOutputTokens": max_tokens,
                            "temperature": temperature
                        }
                    }
                )
                response.raise_for_status()
                return response.json()["candidates"][0]["content"]["parts"][0]["text"]
            except Exception as e:
                logger.error(f"Google generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if Google API is accessible"""
        return bool(self.api_key)


class CustomProvider(BaseLLMProvider):
    """Custom LLM endpoint provider (OpenAI-compatible)"""

    def __init__(self, api_key: Optional[str], endpoint: str, model: str = "default", **kwargs):
        super().__init__(api_key=api_key, endpoint=endpoint)
        self.model = model

    async def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """Generate text using custom endpoint (OpenAI-compatible format)"""
        async with httpx.AsyncClient(timeout=120.0) as client:
            headers = {"Content-Type": "application/json"}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            try:
                response = await client.post(
                    f"{self.endpoint}/chat/completions",
                    headers=headers,
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
            except Exception as e:
                logger.error(f"Custom provider generation error: {e}")
                raise

    async def is_available(self) -> bool:
        """Check if custom endpoint is accessible"""
        if not self.endpoint:
            return False
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                response = await client.get(f"{self.endpoint}/models")
                return response.status_code in [200, 401]  # 401 means endpoint exists but needs auth
            except:
                return False


class LLMService:
    """
    Unified LLM Service for script generation

    Handles:
    - Multiple provider support
    - Smart prompt engineering for video scripts
    - Duration-aware script generation
    - Script refinement and adjustment
    """

    PROVIDERS = {
        "ollama": OllamaProvider,
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
        "google": GoogleProvider,
        "custom": CustomProvider,
    }

    # Average words per second for duration estimation (adjustable per language)
    WORDS_PER_SECOND = {
        "en": 2.5,
        "es": 2.3,
        "fr": 2.4,
        "de": 2.2,
        "it": 2.4,
        "pt": 2.3,
        "hi": 2.8,
        "zh": 3.5,  # Characters per second for Chinese
        "ja": 3.0,
        "ko": 2.8,
    }

    def __init__(self):
        self.ollama_provider = OllamaProvider(model="llama3.2:3b")

    def get_provider(self, config: LLMProvider) -> BaseLLMProvider:
        """Get provider instance from config"""
        provider_class = self.PROVIDERS.get(config.type)
        if not provider_class:
            raise ValueError(f"Unknown provider type: {config.type}")

        return provider_class(
            api_key=config.api_key,
            endpoint=config.endpoint,
            model=config.model
        )

    def estimate_duration(self, text: str, language: str = "en") -> float:
        """Estimate audio duration for text"""
        wps = self.WORDS_PER_SECOND.get(language, 2.5)
        word_count = len(text.split())
        return word_count / wps

    def estimate_word_count(self, duration: float, language: str = "en") -> int:
        """Estimate word count for target duration"""
        wps = self.WORDS_PER_SECOND.get(language, 2.5)
        return int(duration * wps)

    def _build_script_prompt(
        self,
        segments: List[SegmentDescription],
        style: ScriptStyle,
        video_title: Optional[str] = None,
        video_context: Optional[str] = None,
        custom_instructions: Optional[str] = None,
        fit_to_duration: bool = True
    ) -> str:
        """Build the prompt for script generation"""

        # Calculate target word counts for each segment
        segment_specs = []
        for i, seg in enumerate(segments):
            duration = seg.end_time - seg.start_time
            target_words = self.estimate_word_count(duration, style.language)

            segment_specs.append(
                f"Segment {i+1}:\n"
                f"  - Time: {seg.start_time:.1f}s to {seg.end_time:.1f}s ({duration:.1f} seconds)\n"
                f"  - Description: {seg.description}\n"
                f"  - Target word count: ~{target_words} words" +
                (f"\n  - Keywords: {', '.join(seg.keywords)}" if seg.keywords else "")
            )

        prompt = f"""You are an expert video script writer. Generate narration scripts for a video.

**VIDEO INFORMATION:**
{f'Title: {video_title}' if video_title else ''}
{f'Context: {video_context}' if video_context else ''}

**STYLE REQUIREMENTS:**
- Tone: {style.tone}
- Style: {style.style}
- Target audience: {style.audience}
- Length preference: {style.length}
- Language: {style.language}

**SEGMENTS TO WRITE:**
{chr(10).join(segment_specs)}

**CRITICAL INSTRUCTIONS:**
1. Write a separate script for EACH segment
2. {"IMPORTANT: Each script MUST fit within its time limit. Use the target word count as a guide." if fit_to_duration else "Write natural-length scripts without strict word limits."}
3. Scripts should flow naturally and be engaging
4. Match the tone and style specified
5. Do NOT include timestamps or segment labels in the scripts themselves
6. Write in a way suitable for voice-over narration

{f'**ADDITIONAL INSTRUCTIONS:** {custom_instructions}' if custom_instructions else ''}

**RESPONSE FORMAT:**
Respond with a JSON array containing objects for each segment:
```json
[
  {{"segment": 1, "script": "Your narration text here..."}},
  {{"segment": 2, "script": "Your narration text here..."}}
]
```

Write compelling, engaging narration scripts now:"""

        return prompt

    async def generate_scripts(
        self,
        provider_config: LLMProvider,
        segments: List[SegmentDescription],
        style: ScriptStyle,
        video_title: Optional[str] = None,
        video_context: Optional[str] = None,
        custom_instructions: Optional[str] = None,
        fit_to_duration: bool = True,
        max_retries: int = 2
    ) -> Tuple[List[GeneratedScript], List[str]]:
        """
        Generate scripts for video segments

        Returns:
            Tuple of (generated scripts, warnings)
        """
        provider = self.get_provider(provider_config)
        prompt = self._build_script_prompt(
            segments, style, video_title, video_context,
            custom_instructions, fit_to_duration
        )

        warnings = []
        scripts = []

        for attempt in range(max_retries + 1):
            try:
                response = await provider.generate(prompt, max_tokens=2000, temperature=0.7)

                # Parse JSON from response
                parsed_scripts = self._parse_script_response(response, len(segments))

                for i, (seg, script_text) in enumerate(zip(segments, parsed_scripts)):
                    duration = seg.end_time - seg.start_time
                    estimated_duration = self.estimate_duration(script_text, style.language)
                    word_count = len(script_text.split())
                    fits = estimated_duration <= duration * 1.1  # 10% tolerance

                    scripts.append(GeneratedScript(
                        segment_index=i,
                        start_time=seg.start_time,
                        end_time=seg.end_time,
                        text=script_text,
                        estimated_duration=estimated_duration,
                        word_count=word_count,
                        fits_duration=fits,
                        confidence=0.85 if fits else 0.6
                    ))

                    if not fits and fit_to_duration:
                        overflow = estimated_duration - duration
                        warnings.append(
                            f"Segment {i+1}: Script is ~{overflow:.1f}s longer than segment. "
                            f"Consider shortening or extending the segment."
                        )

                # If all scripts fit or we're not fitting to duration, we're done
                if not fit_to_duration or all(s.fits_duration for s in scripts):
                    break

                # If some scripts don't fit and we have retries left, try again with adjusted prompt
                if attempt < max_retries:
                    # Add more specific length constraints to prompt
                    custom_instructions = (custom_instructions or "") + \
                        " IMPORTANT: Previous attempt had scripts that were too long. Please write shorter, more concise narration."
                    prompt = self._build_script_prompt(
                        segments, style, video_title, video_context,
                        custom_instructions, fit_to_duration
                    )
                    scripts = []
                    logger.info(f"Retrying script generation (attempt {attempt + 2}/{max_retries + 1})")

            except Exception as e:
                logger.error(f"Script generation error: {e}")
                if attempt == max_retries:
                    raise
                await asyncio.sleep(1)

        return scripts, warnings

    def _clean_response(self, response: str) -> str:
        """Clean LLM response - remove thinking tags and other artifacts"""
        # Remove <think>...</think> tags (used by qwen, deepseek, etc.)
        cleaned = re.sub(r'<think>[\s\S]*?</think>', '', response, flags=re.IGNORECASE)
        # Remove other common artifacts
        cleaned = re.sub(r'<\|.*?\|>', '', cleaned)  # Remove special tokens
        cleaned = cleaned.strip()
        return cleaned

    def _parse_script_response(self, response: str, expected_count: int) -> List[str]:
        """Parse LLM response to extract scripts"""
        # First clean the response
        response = self._clean_response(response)

        # Try to extract JSON from response
        try:
            # Find all JSON arrays and pick the best one
            candidates = []
            for match in re.finditer(r'\[[\s\S]*?\]', response):
                try:
                    candidate_text = match.group()
                    data = json.loads(candidate_text)
                    if isinstance(data, list) and len(data) > 0:
                        scripts = []
                        for item in data:
                            if isinstance(item, dict):
                                scripts.append(item.get("script", item.get("text", "")))
                            elif isinstance(item, str):
                                scripts.append(item)

                        if scripts:
                            # Check for placeholder content
                            all_scripts_lower = ' '.join(scripts).lower()
                            is_placeholder = any(phrase in all_scripts_lower for phrase in [
                                'your narration', 'narration text here', 'script here',
                                'your script here', '[your', 'placeholder'
                            ])
                            if not is_placeholder:
                                total_len = sum(len(s) for s in scripts)
                                candidates.append((scripts, total_len))
                            else:
                                logger.debug("Skipping placeholder JSON in response")
                except json.JSONDecodeError:
                    continue

            # Return the candidate with the most content
            if candidates:
                best = max(candidates, key=lambda x: x[1])
                logger.info(f"Parsed {len(best[0])} scripts with {best[1]} total chars")
                return best[0]

        except Exception as e:
            logger.debug(f"JSON extraction failed: {e}")

        # Fallback: split by segment markers
        parts = re.split(r'Segment \d+:', response, flags=re.IGNORECASE)
        parts = [p.strip() for p in parts if p.strip()]

        if len(parts) >= expected_count:
            return parts[:expected_count]

        # Last fallback: return the whole response as single script
        return [response.strip()] * expected_count

    async def refine_script(
        self,
        provider_config: LLMProvider,
        original_text: str,
        target_duration: float,
        current_duration: float,
        instruction: str,
        custom_feedback: Optional[str] = None,
        language: str = "en"
    ) -> Tuple[str, float]:
        """
        Refine a script based on feedback

        Returns:
            Tuple of (refined text, estimated duration)
        """
        provider = self.get_provider(provider_config)

        if instruction == "shorten":
            target_words = self.estimate_word_count(target_duration, language)
            action = f"Shorten this script to approximately {target_words} words while keeping the key message."
        elif instruction == "lengthen":
            target_words = self.estimate_word_count(target_duration, language)
            action = f"Expand this script to approximately {target_words} words with more detail."
        elif instruction == "rephrase":
            action = "Rephrase this script while keeping the same length and meaning."
        elif instruction == "simplify":
            action = "Simplify this script using simpler words while keeping the same length."
        elif instruction == "make_formal":
            action = "Make this script more formal and professional while keeping the same length."
        elif instruction == "make_casual":
            action = "Make this script more casual and conversational while keeping the same length."
        else:
            action = "Improve this script."

        prompt = f"""Refine the following video narration script.

**Current Script:**
{original_text}

**Current Duration:** ~{current_duration:.1f} seconds
**Target Duration:** ~{target_duration:.1f} seconds

**Task:** {action}

{f'**Additional Feedback:** {custom_feedback}' if custom_feedback else ''}

**Instructions:**
- Maintain the core message and intent
- Keep it suitable for voice-over narration
- Return ONLY the refined script text, no explanations

**Refined Script:**"""

        response = await provider.generate(prompt, max_tokens=500, temperature=0.6)
        refined_text = response.strip()
        estimated_duration = self.estimate_duration(refined_text, language)

        return refined_text, estimated_duration

    async def check_health(self) -> Dict:
        """Check availability of all LLM providers"""
        health = {
            "ollama_available": False,
            "ollama_models": [],
            "openai_configured": False,
            "anthropic_configured": False,
            "google_configured": False,
        }

        # Check Ollama
        try:
            health["ollama_available"] = await self.ollama_provider.is_available()
            if health["ollama_available"]:
                models = await self.ollama_provider.list_models()
                health["ollama_models"] = [m.name for m in models]
        except:
            pass

        return health

    def _build_auto_segment_prompt(
        self,
        video_duration: float,
        video_description: str,
        style: ScriptStyle,
        video_title: Optional[str] = None,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0,
        custom_instructions: Optional[str] = None,
    ) -> str:
        """Build prompt for automatic segment creation"""

        # Calculate recommended number of segments
        avg_segment = (min_segment_duration + max_segment_duration) / 2
        recommended_segments = max(1, int(video_duration / avg_segment))

        prompt = f"""You are an expert video script writer and editor. Your task is to analyze a video description and automatically create optimally-timed segments with narration scripts.

**VIDEO INFORMATION:**
{f'Title: {video_title}' if video_title else ''}
Total Duration: {video_duration:.1f} seconds ({video_duration/60:.1f} minutes)
Description: {video_description}

**STYLE REQUIREMENTS:**
- Tone: {style.tone}
- Style: {style.style}
- Target audience: {style.audience}
- Length preference: {style.length}
- Language: {style.language}

**SEGMENT CONSTRAINTS:**
- Minimum segment duration: {min_segment_duration} seconds
- Maximum segment duration: {max_segment_duration} seconds
- Recommended number of segments: {recommended_segments} (adjust based on content)
- Total coverage should be close to but not exceed {video_duration} seconds

**YOUR TASK:**
1. Analyze the video description to identify logical segments/chapters
2. Create appropriate timing for each segment that fits the content
3. Write engaging narration scripts that fit within each segment's duration
4. Ensure scripts are suitable for voice-over narration

**WORD COUNT GUIDE (for duration fitting):**
- 5 seconds = ~12 words
- 10 seconds = ~25 words
- 15 seconds = ~37 words
- 20 seconds = ~50 words
- 30 seconds = ~75 words

{f'**ADDITIONAL INSTRUCTIONS:** {custom_instructions}' if custom_instructions else ''}

**RESPONSE FORMAT:**
Respond ONLY with a valid JSON array. No other text before or after.
Each object must have these exact fields:
```json
[
  {{
    "name": "Introduction",
    "start_time": 0.0,
    "end_time": 10.0,
    "description": "Brief description of what happens in this segment",
    "script": "Your narration text here that fits within the time limit..."
  }},
  {{
    "name": "Main Content",
    "start_time": 10.0,
    "end_time": 25.0,
    "description": "Brief description...",
    "script": "Narration text..."
  }}
]
```

Create the segments now:"""

        return prompt

    async def auto_generate_segments(
        self,
        provider_config: LLMProvider,
        video_duration: float,
        video_description: str,
        style: ScriptStyle,
        video_title: Optional[str] = None,
        min_segment_duration: float = 5.0,
        max_segment_duration: float = 30.0,
        custom_instructions: Optional[str] = None,
        max_retries: int = 2
    ) -> Tuple[List[AutoGeneratedSegment], List[str]]:
        """
        Automatically generate segments with timing and scripts from video description

        Returns:
            Tuple of (generated segments, warnings)
        """
        provider = self.get_provider(provider_config)
        prompt = self._build_auto_segment_prompt(
            video_duration, video_description, style, video_title,
            min_segment_duration, max_segment_duration, custom_instructions
        )

        warnings = []
        segments = []

        for attempt in range(max_retries + 1):
            try:
                response = await provider.generate(prompt, max_tokens=3000, temperature=0.7)

                # Parse the response
                parsed_segments = self._parse_auto_segment_response(response, video_duration)

                for i, seg_data in enumerate(parsed_segments):
                    duration = seg_data["end_time"] - seg_data["start_time"]
                    script_text = seg_data.get("script", "")
                    estimated_duration = self.estimate_duration(script_text, style.language)
                    word_count = len(script_text.split())
                    fits = estimated_duration <= duration * 1.1  # 10% tolerance

                    segments.append(AutoGeneratedSegment(
                        segment_index=i,
                        name=seg_data.get("name", f"Segment {i+1}"),
                        start_time=seg_data["start_time"],
                        end_time=seg_data["end_time"],
                        description=seg_data.get("description", ""),
                        script=script_text,
                        estimated_duration=estimated_duration,
                        word_count=word_count,
                        fits_duration=fits
                    ))

                    if not fits:
                        overflow = estimated_duration - duration
                        warnings.append(
                            f"Segment '{seg_data.get('name', i+1)}': Script is ~{overflow:.1f}s longer than segment duration."
                        )

                # Validate segments don't overlap and are within video duration
                for i in range(len(segments) - 1):
                    if segments[i].end_time > segments[i+1].start_time:
                        warnings.append(
                            f"Segments '{segments[i].name}' and '{segments[i+1].name}' overlap."
                        )

                if segments and segments[-1].end_time > video_duration:
                    warnings.append(
                        f"Last segment extends beyond video duration ({segments[-1].end_time:.1f}s > {video_duration:.1f}s)"
                    )

                break

            except Exception as e:
                logger.error(f"Auto-segment generation error (attempt {attempt + 1}): {e}")
                if attempt == max_retries:
                    raise
                await asyncio.sleep(1)

        return segments, warnings

    def _parse_auto_segment_response(self, response: str, video_duration: float) -> List[Dict]:
        """Parse LLM response for auto-generated segments"""
        # Clean the response first
        response = self._clean_response(response)

        try:
            # Find JSON array in response
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                data = json.loads(json_match.group())
                segments = []

                for item in data:
                    if isinstance(item, dict):
                        # Validate required fields
                        start_time = float(item.get("start_time", 0))
                        end_time = float(item.get("end_time", start_time + 10))

                        # Ensure end_time doesn't exceed video duration
                        end_time = min(end_time, video_duration)

                        # Ensure valid timing
                        if end_time <= start_time:
                            end_time = min(start_time + 10, video_duration)

                        segments.append({
                            "name": item.get("name", f"Segment {len(segments) + 1}"),
                            "start_time": start_time,
                            "end_time": end_time,
                            "description": item.get("description", ""),
                            "script": item.get("script", item.get("text", ""))
                        })

                if segments:
                    return segments

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")

        # Fallback: create a single segment covering the whole video
        logger.warning("Failed to parse segments, creating fallback single segment")
        return [{
            "name": "Full Video",
            "start_time": 0,
            "end_time": video_duration,
            "description": "Complete video narration",
            "script": response.strip()[:500]  # Use first 500 chars of response as script
        }]


# Singleton instance
llm_service = LLMService()
